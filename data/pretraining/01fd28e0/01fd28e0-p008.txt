X-SPANFORMER
SPAN-AwARE ENCODER
Mean pooling: A simple, position-invariant aggregation computed as the average of con- stituent token vectors:
Sij
hk. j -1+1 k==i
This approach is computationally efficient; robust to span length; and has proven effective in prior span-focused architectures such as BiDAF and SpanBERT [15, 32]. Local self-attention: A lightweight transformer block operates over the token subsequence Hli:j], enabling the model to capture internal asymmetries and intra-span dependencies: Sij SelfAttn( H[i:j]). This mirrors span-centric refinement modules in neural coreference and sequence segmentation models [34, 20], offering higher expressivity at moderate computational cost.
In practice, both methods can be fused or gated dynamically based on span type or predicted length, enabling flexibility in balancing generalization and expressiveness across heterogeneous spans.
3.6 Discrete Integration
In the default architecture, span embeddings are appended to the encoder input to form an aug- mented composite sequence: E = [e1, - eL, Si1j1)* Sikir ] â‚¬ R(L+K)xd. This construction allows learned spans to act as soft pseudo-tokens (discrete units with internal semantics  derived from upstream  modules)   which are then  jointly contextualized   alongside the original input. Similar forms of auxiliary token insertion have been shown to improve performance in span-level tasks; prompt tuning, and cross-modal fusion [35, 30, 36]. Importantly; standard transformer encoders can process this composite sequence without architec- tural modification, as the inserted vectors match token dimensionality and participate in multi-head attention identically [5, 7, 35]. This approach mirrors the insertion of learned prompt tokens Or re- trieved vectors into encoder-decoder models without altering core attention mechanics. However,; to prevent positional ambiguity and enforce separation between token and span-originated embeddings, we optionally apply specialized feature encodings (such as segment tags, span-type biases; o learned offsets) to preserve structural grounding [30, 37, 18].
Relative offsets: Span positions can be encoded relative to the input sequence to model anchoring [37]. Segment or modality tags: Type embeddings help disambiguate pseudo-tokens originating from different sources Or domains [18, 20]. Attention masking: Optionally restrict cross-token attention during pretraining to reduce information bleed or enforce compositional constraints
While this discrete integration is straightforward in implementation, it serves as a critical interface between low-level segmentation logic and high-level fusion O reasoning stages Alternative strategies such as early fusion; cross-attention bridging, O gating will be explored in the upcoming sections.
9