X-SPANFORMER
SPAN-AwARE ENCODER
X
X
X
Text
Dense
Image
Click
Prefix Token
Speech
Set
Video
Multimodal
Fused Span Vector S
Attention Bias
X
8 L
Transformer encoder
Prefix
S
X2
Prefix token
Prefix token
Xi X
Gating Vector
Attention W bias Wg Gating- vector
Transformer encoder
X
Controller-Aware Generation
Auto-Routed Transformer
Gecior Wg
MLP
MLP
Figure 4: Diagram of span controller injection pathways. The fused control vector s is injected at various stages of the transformer stack via prefix tokenization; attention projection, or feed-forward gating: Each pathway supports differentiable influence over structure-aware representation learning:
Inspired by prefix tuning [30], adapter routing [47], and conditional computation frameworks such as Primer [57] and Galactica [58], we implement three complementary controller injection modes:
(a) Prefix token: s is inserted as a synthetic token at input position t = 0, forming an augmented sequence: X = [s, 81, 12, 8 L ], allowing early layers to attend over structure-induced context from the very first step [30]. (b) Attention bias: s is projected via learnable matrices and added to the query key representa- tions before computing attention weights: Qi < Qi + WQ ;, K; < Kj+ WK ;, forming low-rank adaptive adjustments to the attention mechanism [47]. (c) Gating vector: Feed-forward activations are modulated by span-conditioned gates: FFN(h) = o(Wgs) MLP(h), where o is an activation function (e-g , sigmoid Or swish) and denotes elementwise multipli- cation: This enables multiplicative control over token-wise representations.
Each controller pathway biases computation differently: prefix tokens affect token flow from the input layer , attention projection adjusts mid-layer relational processing; and gating modulates late stage nonlinearity: These methods may be used independently or composed with learned scalar weights Or routing heuristics
Semantic Routing Interpretation. The use of $ as a dynamic structure-derived signal resembles latent prompting or universal adaptation: Related techniques include T adapters [46], PADA [59], prefix routing [28], and memory-based policies [60]. Unlike prior works; X-Spanformer constructs S from differentiable span selection instead of metadata or fixed domain tags.
17