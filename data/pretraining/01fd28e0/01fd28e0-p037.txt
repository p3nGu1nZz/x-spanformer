X-SPANFORMER
SPAN-AwARE ENCODER
3
Extended Ablation Settings
Fusion head variants: Compared MLP (w) VS. LayerNorm(MLP) for @k scoring: Gated units improved stability in low-entropy regimes. Routing depth: Explored controller depth dc â‚¬ {1,2,3}; performance plateaued beyond dc = 2.
Gradient gating: Evaluated freezing   fe for first 5 epochs to encourage stable Lent decay: Marginal performance trade-off observed. Span type probing:  Used auxiliary decoders (e-g-, NER, chunking) as structural supervision for P gold in Equation (2?). Slight gains in low-resource settings. Span pooling alternatives: Replaced gated attention with mean/max pooling for spans; gated attention retained higher semantic alignment (measured by cosine with target label embeddings).
References
[1] Rico Sennrich, Barry Haddow, and Alexandra Birch. "Neural Machine Translation of Rare Words with Subword Units". In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics  (Volume 1: Long Papers). Berlin, Germany: Association for Computational Linguistics, 2016, Pp. 1715-1725. DOI: 10.18653/v1/P16- 1162. URL: https = //aclanthology org/P16 - 1162. [2] Taku Kudo and John Richardson: (( SentencePiece: A simple and language independent sub- word tokenizer and detokenizer for Neural Text Processing". In: Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Processing: System Demonstrations. Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 66-71. DOI: 10.18653/v1/D18 - 2012. URL: https: / /aclanthology.org/D18-2012. [3] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly: (( Pointer  Networks" _ In: Advances  in Neural Information Processing Systems. Vol: 28. 2015, pp_ 2692-2700. URL: https I/arxiv _ org/abs/1506.03134. 4] Ashish Vaswani et al. (( Attention Is All You Need" . In: Advances in Neural Information Pro- cessing Systems: Vol: 30. 2017, pp_ 5998-6008. URL: https: / /arxiv.org/abs/1706.03762. [5] Jacob Devlin et al. (( BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, 2019, pp. 4171-4186. DOI: 10.18653/v1/119-1423. URL: https 1 /aclanthology.org/N19-1423. [6] Alec Radford et al. Language Models are Unsupervised Multitask Learners. OpenAI Technical Report. Available at https 1 | cdn openai com / better language models language _ models_are_unsupervised_multitask_learners.pdf_ 2019. [7] Colin Raffel et al. (( 'Exploring the Limits of Transfer Learning with a Unified Text-toText Transformer' In: Journal of Machine Learning Research 21.140 (2020), Pp 1-67 . URL: https [ljmlr.org/papers/v21/20-074.html. [8] Michiel de Galle, Benoit Sagot, and Djame Seddah: Respite: A Tokenization-Free Multilingual Language Model". In: Proceedings of EMNLP 2021. 2021, pp. 288-302.
38