X-Spanformer: A Tokenizer-Free, Span-Aware Encoder Inspired by X-Bar Theory
Kara Marie Rawson`
Aimee Chrzanowskit
June 26, 2025
This work %8 & preprint and has not yet been peer reviewed.
Abstract
Tokenization remains a limiting factor in contemporary transformer architectures, typically grounded in static subword vocabularies that generalize poorly across heterogeneous or evolving textual inputs. We introduce X-Spanformer, a tokenizer-free segmentation module that re places heuristic lexical boundaries with dynamic span prediction inspired by X-bar theory: The model initializes with a compact lk-unit BPE vocabulary [1, 2] and transitions to span-based segmentation via a pointer network [3], supervised through an annealed auxiliary loss schedule Predicted spans are variable-length, overlapping, softly typed by modality (e-8-, code, natural language, identifier) , and dynamically capped per input using a learned span length estimator: Span representations are composed and injected into downstream encoders, allowing seamless integration with pretrained or co-trained transformer stacks We hypothesize that learned span prediction provides more semantically aligned and compression-efficient tokenization than fixed BPE or byte-level alternatives. To investigate this; we construct a multi-phase curriculum that bootstraps from synthetic segmentation labels and gradually introduces stream-type-aligned supervision: We detail evaluation protocols for measuring compression ratio contrastive re- trieval alignment, span entropy; and modality coherence   Preliminary results suggest improved interpretability and structural alignment, supporting our hypothesis. We release a standalone ONNX-compatible implementation along with training recipes and COrpus construction guide lines to facilitate adoption across code, language, and hybrid domains
1
Introduction
Transformer architectures underpin leading solutions in natural language understanding; program synthesis; and multimodal retrieval [4, 5, 6, 7]. Central to these models is a static segmentation stage that partitions input into fixed subword units, most commonly via Byte-Pair Encoding [1] or SentencePiece [2]. While effective on in-domain corpora, these pipelines impose immutable lexical boundaries that degrade under domain shift, obscure long-range compositional patterns in code and multilingual text [8], and incur substantial costs when vocabularies must be revised for novel syntactic O semantic phenomena. rawsonkara @gmail.com T aimeechrzanowski@gmail com
2