X-SPANFORMER
SPAN-AwARE ENCODER
Algorithm 1 Phase I Span Pretraining Require: Dataset D = {(z() , y())}N1; scorer fo; aggregator 9o 1: for each batch (1,y) in D do 2: Sample spans (i,3); mask region Ti:j 3: Compute pooled span embedding sk Pool(.i:j) 4: Predict reconstruction Ti:j = decode(go(8k)) 5: Evaluate: Crecon 3 Ilwi:j ci:jllg or token-wise cross-entropy 6: Backpropagate and update 0, 7 end for
This step biases the model toward identifying spans that support local coherence, compression, or predictive fidelity   Entropy regularization is applied to the span scorer during this phase with constant weight AO; maximizing routing diversity as in [52].
Phase Il: End-to-End Fine-Tuning (Joint Routing + Representation)
Once the span routing mechanism has  converged on stable inductive patterns; we integrate the controller vector 3 into the transformer encoder and perform full-model training:
=
Ltotal Ltask + B1~ Lspan + 82Lent,
(27)
where Ltask is the downstream loss (e:g-, NLL, classification, contrastive loss), Lspan is a KL di- vergence against interpretable span supervision (when available) , and Lent is the Shannon entropy regularizer defined previously: The entropy coefficient is annealed exponentially:
Aent ' (t) = Ao exp(_y ' (t _ Ti)) ~ 1+7T1,
(28
where Ti marks the transition epoch from Phase I, and 7 modulates the sharpness of routing focus:
Algorithm 2 Phase II Full-Model Optimization Require: Trained fo, 9o; transformer %; entropy schedule Aent (t) 1: for epoch t = Ti + 1 to T2 do 2: for each batch (x,y) do 3: Compute span logits: Wk 9o (8k, `); @k softmax(Wk) 4: Fuse: S = k @kSk 5: Inject s via prefix, bias, O gate (see Section 4.3) 6: Compute L using Equation (27) 7: Backpropagate and update 0, 0,1 8: end for 9: end for
Training Summary: Phase I focuses on disentangling structural plausibility from task grounding: Phase II jointly optimizes the full routing-and-reasoning stack using controller fusion as & structural
20