X-SPANFORMER
SPAN-AwARE ENCODER
[42] Manzil Zaheer et al. (( Big Bird: Transformers for Longer Sequences" . In: Advances in Neural Information Processing Systems 33 (2020) , pp. 17283-17297. URL: https : / /arxiv.org/abs/ 2007 14062. [43] Noam Shazeer et al. "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of- Experts Layer" . In: arXiv preprint arXiv:1901.06538 (2017). URL: https : / /arxiv.org/abs/ 1701.06538. [44] Joshua Ainslie et al. "CoLT5: Faster Long-_ Range Transformers with Conditional Computa- tion". In: Proceedings of the 2023 Conference on Empirical Methods  in Natural Language Processing (EMNLP) Singapore: Association for Computational Linguistics, 2023, pp 5085 5100 . URL: https: / /aclanthology.org/2023.emlp-main.309/. [45] Junxian He et al. " Syntax-Enhanced Transformer for Neural Machine Translation". In: arXin preprint arXiv:2002.01160 (2020). URL: https: / /arxiv_ org/abs/2002.01160. [46] Colin Raffel et al. "Exploring the Limits of Transfer Learning with a Unified Text-toText Transformer" . In: Journal of Machine Learning Research 21.140 (2020) , pp. 1-67. URL: https: / / jmlr.org/papers/v21/20-074.html. [47] Edward J. Hu et al. (( LoRA: Low-Rank Adaptation of Large Language Models" . In: arXi: preprint arXiv:2106.09685 (2021). URL: https: / /arxiv.org/abs/2106.09685. [48] Mike Lewis et al. "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation; and Comprehension". In: Proceedings of ACL. 2020, Pp. 7871-7880. URL: https //aclanthology.org/2020 .acl-main.703. [49] Andre FT Martins et al. ((  Latent Structure Models for Natural Language Processing" . In: Proceedings  of the 5 rth Annual Meeting  of the Association for Computational Linguistics: Tutorial Abstracts. 2019, pp. 1-5. [50] Chenchen Ma, Jing Ouyang, and Gongjun Xu: (( Learning Latent and Hierarchical Structures in Cognitive Diagnosis Models" . In: Psychometrika 88.1 (2023) , Pp: 175-207. DOI: 10 _ 1007 / s11336-022-09867-5. [51] Yi Tay et al. "Efficient Content-Based Sparse Attention with Routing Transformers". In: Trans- actions of the Association for Computational Linguistics 9 (2021) , pp. 53-68. DOI: 10.1162/ tacll_a|_00353. [52] Yves Grandvalet and Yoshua Bengio. (( Semi-Supervised Learning by Entropy Minimization". In: Advances in Neural Information Processing Systems. 2005, Pp. 529-536. [53] Gabriel Pereyra et al. "Regularizing Neural Networks by Penalizing Confident Output Distri- butions"  In: International Conference on Learning Representations (ICLR): 2017. [54] Yoshua Bengio et al. "Curriculum Learning" In: Proceedings of the %6th Annual International Conference on Machine Learning: 2009, Pp 41 48. [55] Andrew Drozdov et al. (( Unsupervised Latent Tree Induction with Deep Inside-Outside Recur- sive Autoencoders" . In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics; 2019, Pp. 1129-1141. URL: https: / /aclanthology org/N19- 1116/. [56] Kevin Clark et al. "Semi-Supervised Sequence Modeling with Cross-View Training". In: Pro- ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: As- sociation for Computational Linguistics, 2018, Pp 1914-1925. URL: https 1 /aclanthology org/D18- 1217/-
41