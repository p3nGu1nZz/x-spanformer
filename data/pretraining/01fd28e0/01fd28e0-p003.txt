X-SPANFORMER
SPAN-AwARE ENCODER
2.2
Character-Level and Token-Free Models
To mitigate subword brittleness, several works propose bypassing static vocabularies in favor of character- or byte-driven   encoding: Charformer applies gradient-based subword tokenization to learn latent splits during pretraining; compressing sequences without a predefined vocabulary [9]. CANINE directly encodes Unicode codepoints with down-sampling and up-sampling layers, offering a tokenization-free encoder that matches BPE baselines on transfer tasks [10]. These approaches remove offline heuristics; but they do not explicitly model higher-order linguistic 0 symbolic struc tures.
2.3 Unsupervised and Differentiable Segmentation
Beyond character-level models; unsupervised segmentation methods aim to learn meaningful units directly from raw streams. Morfessor induces morpheme-like units via minimum description length objectives [14]. In the neural domain, probabilistically masked language models (PMLM) inte- grate segmentation into pretraining with masked span prediction [12]. Other works learn segmen- tation boundaries for text-to-text generation by optimizing a downstream reconstruction loss [11]. While these methods introduce differentiability; they lack   explicit linguistic priors and produce non-overlapping, fixed partitions:
2.4 Span-Based and Pointer-Network Models
Pointer networks offer & mechanism for predicting variable-length spans by emitting start and end indices over an input sequence [3]. SpanBERT extends this idea in pretraining by masking contigu- ous spans and predicting their content [15]. In speech and vision; learned segmenters often output overlapping proposals that improve detection and alignment [16, 17]. However , to our knowledge no prior work unifies pointer-based span prediction with linguistically grounded structure for text segmentation in transformer encoders:
2.5 Summary
Existing  segmentation   strategies  fall into three broad categories: offline   subword  tokenization; character-level or token-free encoders, and unsupervised boundary learners: Offline methods such as BPE and SentencePiece offer efficient lookups but impose static vocabularies that fragment long- range structures and fail under domain shift [1, 2, 8]. Character-level and byte-level approaches eliminate heuristic preprocessing yet lack explicit modeling of phrase-level regularities and often incur higher  computational cost [9, 10]. Unsupervised segmentation methods introduce differen- tiability but produce non-overlapping; monolithic partitions without linguistic priors [14, 1l, 12]. Span-based predictors and pointer-network architectures enable variable-length boundary propos- als but have not been combined with generative grammar principles for text segmentation [3, 15]. X-Spanformer addresses these gaps by unifying pointer-based span prediction with X-bar inspired inductive bias, yielding overlapping, softly typed spans that integrate seamlessly into transformer encoders and support end-to-end training: