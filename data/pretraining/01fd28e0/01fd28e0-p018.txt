X-SPANFORMER
SPAN-AwARE ENCODER
Final Objective: Let Ltask denote the supervised loss (e-g;, classification Or alignment). The total objective includes structure alignment and entropy-based exploration:
L = Ltask + 81- Lspan + B2Cent =
where /81, 82 € Rzo control regularization strength and structure confidence
Span Induction Top-K expfsxp) spans 8 fe Zexpf Eu 0 Entropy nent H(P) Span Fusion Span fusion
Transformer Backbone
Prefix token
Attention bias
2
Gating vector
€ =2 ak
Figure 5: Training workflow of X-Spanformer: Spans are scored, entropy-regularized , and interpolated into a fused control vector s, which conditions the backbone encoder via multiple integration modes
4.4
Optimization and Curriculum Strategy
X-Spanformer is trained via a structured two-stage curriculum designed to (i) bootstrap structural induction from local compositional statistics, and (ii) fuse these learned inductive biases into an end-to-end transformer backbone. This approach draws from established principles in multi-phase self-supervision [5, 61], curriculum learning [54, 24], entropy-guided latent modeling [55], and gradual architectural fusion [62, 48].
The optimization process proceeds as follows:
Phase I: Span Pretraining (Structure Discovery)
This phase isolates the span scorer fo and aggregator g6 to encourage compositional discovery independent of downstream gradients: The learning objective focuses on reconstruction or type classification:
pre
Lrecon BauxLaux ,
(26)
where Lrecon is a span-wise MSE or token-level cross-entropy loss; and Laux may capture span-type heuristics (e.g-, POS tags; constituency labels) from lightly supervised signals [63]. 1 In our experiments; tuning 81 € [0.5,1.5] yielded consistent gains on structured tasks. Lowering 82 0.3 after warmup preserved sparsity and improved convergence stability:
19