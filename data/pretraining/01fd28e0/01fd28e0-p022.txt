X-SPANFORMER
SPAN-AwARE ENCODER
Gigaword Compression (Optional): For assessing semantic condensation and routing spar- sity under low-token summarization windows [70]_ Pseudo-structured Sequences: A mix of instructional data (recipes, dialog trees) and semi- nested markdown documents to probe structural generalization over latent hierarchical cues:
Metrics. To isolate architectural effects, we evaluate span selection and routing behavior using the following indicators:
Span entropy:
H(P) = - Pij log Pij, (i,j)es to assess structural uncertainty
(32)
Average span width:
U = Eli,j)~P[j - i, indicating the model's preferred compositional grain. Overlap rate: Overlap( B) 1[sk O se = 0], IBl K2 xEB kze where B is a mini-batch; and {Sk} are selected spans per instance.
(33)
Controller gate entropy:
K H(a) = - @k log  @k; k=l
reflecting the distributional sharpness of fused routing signals.
Baselines. To contextualize architectural effects; we compare against:
Vanilla Transformer Encoder: Without span selection Or controller routing; matches embed- ding dimensionality and depth: Prefix-Tuned Transformer [30]: Appends learnable prefix tokens to the input sequence, serv- ing as & lightweight prompting baseline Latent Syntax Attention [55]: Implements unsupervised span-based parse induction using differentiable parsing objectives.
Infrastructure. All experiments are conducted on a single 40GB NVIDIA A10o GPU. Training time per phase is approximately 10-12 hours. Models are implemented in PyTorch and exported uS- ing ONNX traceable modules for architecture inspection and routing visualization: Hyperparameter values are enumerated in Appendix .1.
23