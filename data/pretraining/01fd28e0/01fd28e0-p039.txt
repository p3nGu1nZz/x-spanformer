X-SPANFORMER
SPAN-AwARE ENCODER
[25] Jianpeng Liu et al. C( Table-to-text generation by structure-aware seq2seq learning". In: AAAI. 2018, pp 4881 4888. [26] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Masked-Attention Mask Trans- former for Universal Image Segmentation". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2021, pp. 1290-1299. [27] Zi Lin, Sweta Agrawal, and Smaranda Muresan: (( Learning Cross-lingual Code-switching for Generative Language Models". In: Findings of EMNLP 2021. 2021, pp. 2678-2689. [28] Jai Gupta et al. (( Molt: Modular Prompt Tuning for Multi-task and Cross-lingual Transfer". In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2022. [29] Daniel Khashabi et al: (( UnifiedQA: Crossing Format Boundaries with a Single QA System". In: Findings of EMNLP 2020. 2020, Pp. 1896 1907. [30] Xiang Lisa Li and Percy Liang: Prefix-Tuning: Optimizing Continuous Prompts for Gen- eration" . In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL) 2021, pp. 4582-4597. [31] Zi Lin et al. (( GLAIVE: Global Context Aware Generation for Code-Mixed Dialogues" . In: Findings of ACL 2022. 2022, Pp: 672-685. [32] Kenton Lee, Mike Lewis, and Luke Zettlemoyer . (( End-to-End Neural Coreference Resolution". In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2017. [33] Jianpeng Cheng; Michael Kuehl, and Mirella Lapata. (( Probing What Different NLP Tasks Teach Machines About Function Word Comprehension" . In: Findings of EMNLP. 2020. [34] Kenton Lee et al. Higher-Order Coreference Resolution with Coarse-to-Fine Inference". In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) 2018. [35] Kelvin Guu et al. (( REALM: Retrieval-Augmented Language Model Pre-Training" . In: Pro- ceedings of the 3rth International Conference on Machine Learning (ICML): 2020. [36] Weizhe Zuo et al: L( Rethinking Insertion for Transformer-Based Language Modeling" In: Find- ing8 of ACL. 2022. [37] Peter   Shaw Jakob Uszkoreit, and Ashish   Vaswani   "Self-Attention  with Relative Position Representations" . In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). 2018. [38] Susan Zhang et al. OPT: Open Pre-trained Transformer Language Models" . In: arXiv preprint arXiv:2205.01068 (2022) . URL: https: / /arxiv.org/abs/2205.01068. [39] Gautier Izacard and Edouard Grave. t Distilling Knowledge from Reader to Retriever for Ques- tion Answering . In: Advances in Neural Information Processing Systems (NeurIPS). 2020. [40] Shivangi Arora et al. "ExSum: From Local Explanations to Model Understanding". In: Ad- vances in Neural Information Processing Systems (NeurIPS): 2022. [41] Iz Beltagy; Matthew E. Peters; and Arman Cohan. (( Longformer: The Long-Document Trans- former" . In: arXin preprint arXiv:2004.05150 (2020)_ URL: https 1 arxiv . Org/abs, 2004 05150.
40