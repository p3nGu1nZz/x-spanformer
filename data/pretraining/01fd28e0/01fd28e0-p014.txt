X-SPANFORMER
SPAN-AwARE ENCODER
To encourage   diversity and avoid premature collapse into high-confidence routing, we   apply a temperature-weighted entropy penalty:
Lent
~Aent H(P); H(P) = Pij log Pij: (i,j)es
(17)
This follows the principle of entropy regularization [52, 53], where high-entropy distributions en- courage exploration under uncertainty The regularization strength Aent is annealed exponentially:
Aent (t) = Ao exp( _yt) ,
(18)
where t is the training epoch, Ao the initial coefficient, and ~ 0 controls decay rate This annealing scheme mirrors curriculum learning and gradual constraint tightening in latent modeling [54, 24].
Proposition 6 (Maximum Entropy of Uniform Span Distribution). Let S be  the set of spans defined in Equation (4), with |S| = N. The entropy of the softmax span distribution P, a8 given in Equation (16) , is maximized when:
Pij
for all (i,j) â‚¬ S. N
(19)
In that case, the entropy attains its maximum value:
Hmaxl (P) = log/S| = log N.
(20)
Proof: We wish to maximize:
H(P) = - Pij log Pij, (i,j)es
(21)
subject to the constraints:
Pij = 1, Pij 2 0. (i,j)es
(22)
Form the Lagrangian:
L(P,A) =
Pij log Pij + A Pij " =1 (i,j)es (i,j)es
(23)
The first-order stationarity condition yields: OL log Pij - 1+A = 0 dPij
Pij =eA-1
(24)
Since all Pij are equal and sum to 1, we conclude Pij = 1/N. Substituting into Equation (21):
H(P*) =-N - Nlog n) = log N.
(25
15