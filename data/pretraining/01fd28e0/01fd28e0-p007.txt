X-SPANFORMER
SPAN-AwARE ENCODER
3.4
Modality Typing
Spans in source sequences often originate from heterogeneous subdomains ~such as natural language, programming syntax; structured identifiers, numeric expressions, Or markup. Accurate identification of a span's modality enables the model to apply domain-specialized logic (e:g , routing to type specific heads; enforcing syntax-aware constraints, Or improving retrieval and alignment). 8 To this end, we introduce a shallow classification head that predicts a probability distribution over T predefined modalities for each span: Let ij € Rd be the pooled embedding for span (i, j) , produced by the same pooling operator used in the length estimator:
Uij = Pool( H[i:j]).
We compute logits and a normalized modality distribution:
type ei = WtUij + bt,
type pij softmax( etype
The vector ptype T represents the model's belief over  candidate types, capturing epistemic uncertainty in ambiguous or code-switched contexts [27]. This representation serves three purposes:
1. Auxiliary Supervision: When modality annotations are available, a croSS-entropy loss be tween ptype and gold labels provides an auxiliary training signal. This approach aligns with multitask conditioning frameworks like UnifiedQA [29] and improves zero-shot transfer in underrepresented types [28]. 2 . Stream Conditioning: During decoding O1 croSS-_ attention, type distributions can be used to guide soft routing among specialized decoder blocks O1  prompt adapters [30], enabling modular reasoning across modalities: 3. Interpretability and Diagnostics: The modality classifier enhances model transparency by linking segmentation decisions to functional subdomains especially in token-level mixtures or generated scaffolding tasks: 9
3.5 Span Embedding
Each retained span (i,j) € S' must be mapped to a fixed-size vector representation suitable for downstream fusion: This embedding is intended to capture both the internal structure and the contextual salience of the span, serving aS a condensed representation of its semantic Or syntactic role. Effective span  encodings have been  shown to improve performance in question answering, entity linking; and structured generation tasks [32, 15, 33]. We  explore two span encoding strategies, inspired by prior work in segment pooling and local contextualization: See [27, 28] for techniques that use token- or span-level typing to improve generative fluency and interpretability in mixed-modality environments. 9This is particularly useful for diagnosing over-segmentation, ambiguous boundaries, Or routing errors in compo- sitional data streams [31, 20].