X-SPANFORMER
SPAN-AwARE ENCODER
Proposition 3 (Equivariance and Convexity). Let S' be any permutation of filtered spans: Then the interpolated vector $ is:
1 . Permutation equivariant: invariant to reordering of spans i S'_
2 Differentiable: gradients propagate through both Wij and Sij, 3 Convex: 8 € conv{8ij (i,j) € S}.
Proof   Equivariance follows from the input-order invariance of softmax in Equation 3. Differen- tiability holds because both the scoring function and span encodings are differentiable mappings: Convexity arises from expressing $ in Equation 1 as a convex  combination of fixed vectors with weights @ij Z 0, summing to 1.
3.8 Runtime Complexity
key design goal of X-Spanformer is to enhance structural awareness without compromising com- putational efficiency To this end, we decompose the end-to-end forward pass into three core stages:
Span enumeration and scoring: generation and scoring of candidate spans from the input sequence; Embedding and selection of top-ranked spans: pooling span-level representations and selecting & subset for contextual conditioning; Joint contextualization: applying a standard transformer encoder over the combined se - quence of tokens and selected spans:
This modular design ensures that added computational cost remains subquadratic for the first two stages, while the dominant quadratic term scales with total input length:  Similar hybrid strategies are used in sparse attention transformers [41, 42] and routing-based models [43, 44]. The proposition below formalizes the overall runtime cost: Proposition 4 (Runtime Bound) . Let L be the input sequence length; K the number of retained spans, Umax the maximum span width, and d the model's hidden dimension: Then the total forward pass runtime of X-Spanformer is:
O(Lrmax , +O(Kd) + O(L + K)2).
Proof: The total runtime decomposes into the following: (1) Span enumeration and scoring: Each of the L tokens anchors up to Umax rightward spans. Each span is scored by a parameterized function fe(wi:j), typically an MLP o bilinear form. Hence the cost is: 0(Lwmax ,
(2) Span encoding and filtering: After top-K selection, each span is pooled (e:8 , via mean or self-attention) into a vector of dimension d, and scored by span-type and confidence heads: These operations are linear in d, giving: O(Kd).
11