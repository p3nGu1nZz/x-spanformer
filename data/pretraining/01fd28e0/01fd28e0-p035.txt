X-SPANFORMER
SPAN-AwARE ENCODER
7.1 Effect of Span Injection Strategies
We compare the following injection strategies for incorporating $: Prefix Token (PT): Insert 3 at position 0- Attention Bias (AB): Add 8 to keys/ queries linearly as in Section ?2 . Gated FFN (GF): Modulate FFN output via span-conditioned gating: Let Lfull denote the baseline loss with all three injections, and L_m be the loss with mechanism m removed. Define relative degradation m as: L_m Lfull m 3 100% Lfull We expect to observe: PT 1.29, AAB = 2.7%, and AGF = 4.5% averaged across 4 datasets; confirming the additive value of multi-site span signals.
7.2 Span Selection without Confidence Routing
We ablate the confidence-gated routing step and instead use uniform averaging over K top spans. Let: K K Suniform Sk, Sconf = @kSk, @k softmax(9o(sk)) k=l k=1
Proposition 12. Let sk € Rd be fired span vectors and g be Lipschitz continuous. Then EIllsconf Suniform|= 1ll?] 2 0 with equality only if 9o is constant or the spans are identical
Proof: Since softmax is strictly convex, equality occurs iff @k = 1/K for all k, which holds if and only if go (sk) = c for all k. This requires either span homogeneity or trivial 9o-
Empirically, we expect to observe a consistent F1 drop of ~ 2.1% when using Suniform; validating the role of confidence-modulated routing [76].
7.3 Span Pooling Alternatives
We replace Pool(zi:j) with various alternatives: max(xi:j ) max-pooling mean(€i:j) mean-pooling Ii start-token only Our simulated projections predict that mean-pooling will consistently outperformed other meth- ods (up to +1.8% over max)_ This might correlate to to reduced gradient variance and better generalization [76].
7.4 Disabling Span-Scoped Attention
Finally; we ablate the span-aware bias term in attention: epan = Cij + Sijes - B , B €e R
(3)
36