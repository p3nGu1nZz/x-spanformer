# Vocab2Embedding Pipeline Configuration
# This configuration defines parameters for the tokenizer-free span-aware encoding pipeline

# Neural architecture parameters - Section 3.2
architecture:
  # Embedding dimensions for dense representations
  embed_dim: 512

  # Multi-scale convolution kernels - Section 3.2.3
  # Different kernel sizes capture features at multiple spans
  conv_kernels: [ 3, 5, 7 ]

  # Dilation rates for temporal convolution - expanding receptive field
  conv_dilations: [ 1, 2, 4 ]

  # Regularization
  dropout_rate: 0.1

# Span generation parameters - Section 3.1.2  
span_generation:
  # Vocabulary temperature for unigram LM - Section 3.1.2 Equation 1
  tau_vocab: 1e-4

  # Competition temperature for span selection - Section 3.1.2 Equation 2
  tau_comp: 1e-6

# Processing configuration
processing:
  # Device for computation
  device: "cuda" # cuda | cpu (falls back to cpu if cuda unavailable)
  device_id: 0 # GPU device ID for multi-GPU systems

  # Parallel processing workers
  workers: 1 # Number of worker processes for parallel sequence processing

  # Batch processing parameters
  batch_size: 64
  max_sequence_length: 512

# Numerical stability and precision
numerical:
  # Small epsilon for numerical stability in calculations
  epsilon: 1e-12

  # Maximum piece length for vocabulary items - matching jsonl2vocab.yaml
  max_piece_length: 8

# Output configuration  
output:
  # Save intermediate representations for analysis (disable by default for performance)
  save_intermediate: false

  # Chunked storage configuration - saves embeddings in batches for better I/O efficiency
  chunk_size: 100 # Number of sequences per chunk file (overrideable via --chunk-size)

  # OPTIONAL: Seed embeddings (H0) - intermediate representation for analysis
  # Only needed for debugging vocabulary distribution before contextualization
  # Disable by default since only contextual embeddings (H) are needed downstream (saves ~1MB per sequence)
  # Note: Contextual embeddings (H) are ALWAYS saved as they're essential for ALL downstream tasks
  save_seed_embeddings: false

  # Include JSON metadata with embeddings (disable by default - not needed for core tasks)
  save_json_metadata: false

  # Add detailed analysis to outputs (disable by default)
  add_analysis: false

  # PERFORMANCE OPTIMIZATION: Disable soft probability saving by default
  # Soft probabilities (P matrix) are only used to compute seed embeddings
  # Once seed embeddings are computed, soft probs are not needed downstream
  # This saves ~30MB per sequence and significantly speeds up I/O
  # Set to true only when you need to introspect the forward-backward probabilities
  save_soft_probabilities: false
