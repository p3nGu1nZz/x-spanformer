# Vocab2Embedding Pipeline Configuration
# This configuration defines parameters for the tokenizer-free span-aware encoding pipeline

# Neural architecture parameters - Section 3.2
architecture:
  # Embedding dimensions for dense representations
  embed_dim: 512

  # Multi-scale convolution kernels - Section 3.2.3
  # Different kernel sizes capture features at multiple spans
  conv_kernels: [ 3, 5, 7 ]

  # Dilation rates for temporal convolution - expanding receptive field
  conv_dilations: [ 1, 2, 4 ]

  # Regularization
  dropout_rate: 0.1

# Span generation parameters - Section 3.1.2  
span_generation:
  # Vocabulary temperature for unigram LM - Section 3.1.2 Equation 1
  tau_vocab: 1e-4

  # Competition temperature for span selection - Section 3.1.2 Equation 2
  tau_comp: 1e-6

# Processing configuration
processing:
  # Device for computation
  device: "cuda" # cuda | cpu (falls back to cpu if cuda unavailable)
  device_id: 0 # GPU device ID for multi-GPU systems

  # Batch processing parameters
  batch_size: 64
  max_sequence_length: 512

# Numerical stability and precision
numerical:
  # Small epsilon for numerical stability in calculations
  epsilon: 1e-12

  # Maximum piece length for vocabulary items - matching jsonl2vocab.yaml
  max_piece_length: 8

# Output configuration  
output:
  # Save intermediate representations for analysis (disable by default for performance)
  save_intermediate: false

  # Export numpy arrays for downstream processing - ESSENTIAL (always keep these)
  save_numpy_arrays: true

  # Include JSON metadata with embeddings (lightweight, useful for introspection)
  save_json_metadata: true

  # Add detailed analysis to outputs (disable by default)
  add_analysis: false

  # PERFORMANCE OPTIMIZATION: Disable soft probability saving by default
  # Soft probabilities (P matrix) are only used to compute seed embeddings
  # Once seed embeddings are computed, soft probs are not needed downstream
  # This saves ~30MB per sequence and significantly speeds up I/O
  # Set to true only when you need to introspect the forward-backward probabilities
  save_soft_probabilities: false
