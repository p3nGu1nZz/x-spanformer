\subsection{Modality Typing}

Text streams frequently interleave multiple subdomains—natural language, programming syntax, identifiers, numeric literals, or markup. To capture this heterogeneity, we append a lightweight classification head that predicts a soft modality distribution for each span embedding \(v_{ij}\) (pooled as in Section 3.3).

Let \(M\) be the number of modality classes. We first optionally transform the pooled span vector:
\[
h_{ij} = \mathrm{ReLU}\bigl(W_v\,v_{ij} + b_v\bigr)\;\in\;\mathbb{R}^{d'},
\]
with \(d'\le d\).  Then we compute raw logits and probabilities:
\[
\ell^{\mathrm{mod}}_{ij} = W_{\mathrm{mod}}\,h_{ij} + b_{\mathrm{mod}},
\qquad
p^{\mathrm{mod}}_{ij} = \softmax\bigl(\ell^{\mathrm{mod}}_{ij}\bigr)
\;\in\;\Delta^{M}.
\]
Each entry \(p^{\mathrm{mod}}_{ij,m}\) is the model’s belief that span \((i,j)\) belongs to modality \(m\).

To quantify uncertainty, we define the modality entropy
\[
H^{\mathrm{mod}}_{ij}
= -\sum_{m=1}^M
p^{\mathrm{mod}}_{ij,m}\,\log p^{\mathrm{mod}}_{ij,m}.
\]
Higher entropy indicates ambiguous or code-switched spans, which can guide exploration early in training.

This soft‐typing serves three roles:

1. \textbf{Auxiliary supervision.}  
When gold labels \(y^{\mathrm{gold}}_{ij}\in\{0,1\}^M\) are available, we incur a cross‐entropy loss
\[
\mathcal{L}_{\mathrm{mod}}
= -\sum_{(i,j)\in S}
\sum_{m=1}^M
y^{\mathrm{gold}}_{ij,m}\,\log p^{\mathrm{mod}}_{ij,m}.
\]
This signal improves zero-shot transfer and multimodal fluency \cite{khashabi2020unifiedqa, gupta2022molt}.

2. \textbf{Conditional routing.}  
During inference, the distribution \(p^{\mathrm{mod}}_{ij}\) can gate span embeddings through specialized adapters or decoder blocks. For example, a modality embedding
\[
e^{\mathrm{mod}}_{ij}
= p^{\mathrm{mod}}_{ij}\,E_{\mathrm{mod}}
\quad\text{with}\quad
E_{\mathrm{mod}}\in\mathbb{R}^{M\times d}
\]
may be concatenated to \(v_{ij}\) or used to bias attention \cite{li2021prefix}.

3. \textbf{Interpretability.}  
The entropy and class‐probabilities expose which semantic stream each span belongs to, aiding diagnostics in mixed‐modality settings \cite{lin2021codemix, tay2021charformer}.

\medskip
\noindent
\emph{Integration into Span Scoring.}  
We incorporate modality features into the span scoring MLP \(f_{\mathrm{score}}\) used in Section 3.7.  Let
\[
d_{ij} = j - i + 1,\quad
c_{ij} = p^s_i\,p^e_j
\]
be the span length and boundary confidence.  We form the feature vector
\[
x_{ij} = \bigl[
v_{ij};
\phi(d_{ij});
p^{\mathrm{mod}}_{ij};
H^{\mathrm{mod}}_{ij};
c_{ij}
\bigr]\;\in\;\mathbb{R}^{\,d + D + M + 1 + 1},
\]
where \(\phi(d)\in\mathbb{R}^{D}\) is a learned length‐embedding.  Then
\[
w_{ij} = \mathrm{MLP}_{\mathrm{score}}\bigl(x_{ij}\bigr), 
\qquad
a_{ij} = \frac{\exp(w_{ij})}{\sum_{(p,q)\in S'}\exp(w_{pq})}.
\]
This joint conditioning on content, length, type, and confidence yields well‐calibrated relevance weights for overlapping spans.