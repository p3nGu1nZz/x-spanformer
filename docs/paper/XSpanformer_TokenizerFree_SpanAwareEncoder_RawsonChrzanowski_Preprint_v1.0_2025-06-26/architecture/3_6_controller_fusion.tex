\subsection{Controller Fusion}
\label{sec:controller-fusion}

Given the filtered set of span embeddings \(\{s_{ij}\in\mathbb{R}^d\}_{(i,j)\in S'}\), we compute a single fused controller vector \(s\in\mathbb{R}^d\) via relevance‐weighted interpolation. Each span is scored based on its content, type, structure, and boundary confidence.

\subsubsection{Feature Construction and Scoring}

Define the relevance feature for span \((i,j)\in S'\):
\[
x_{ij} = \bigl[
s_{ij};
\phi(\delta_{ij});
p^{\mathrm{mod}}_{ij};
H^{\mathrm{mod}}_{ij};
c_{ij}
\bigr] \in \mathbb{R}^{d + D + M + 1 + 1},
\]
where:
- \(\delta_{ij} = j - i + 1\) is the span length,
- \(\phi(\delta_{ij})\in\mathbb{R}^D\) is a learned embedding,
- \(p^{\mathrm{mod}}_{ij}\in\Delta^M\) is the modality distribution,
- \(H^{\mathrm{mod}}_{ij}\) is the entropy,
- \(c_{ij} = p^s_i\,p^e_j\) is boundary confidence.

Compute normalized relevance weights:
\[
w_{ij} = f_{\mathrm{score}}(x_{ij}),
\quad
a_{ij} = \frac{\exp(w_{ij})}{\sum_{(p,q)\in S'}\exp(w_{pq})}.
\]
Fuse the controller vector:
\[
s = \sum_{(i,j)\in S'} a_{ij} \cdot s_{ij}.
\]

\begin{proposition}[Controller Interpolation Properties]
	Let each \(s_{ij}\in\mathbb{R}^d\) have bounded norm \(\|s_{ij}\|\le B\), and weights \(a_{ij}\in[0,1]\), \(\sum a_{ij}=1\). Then:
	\[
	s \in \mathrm{conv}(\{s_{ij}\}),
	\quad
	\|s\|_2 \le B.
	\]
\end{proposition}

\begin{proof}
	Since \(a_{ij}\) form a probability distribution and all inputs lie in a bounded ball of radius \(B\), their convex combination also lies within that ball:
	\[
	\|s\|_2 = \left\| \sum a_{ij} s_{ij} \right\|_2
	\le \sum a_{ij} \|s_{ij}\|_2 \le \sum a_{ij} B = B.
	\]
\end{proof}

\subsubsection{Transformer Injection Modes}

The controller vector \(s\) is injected into the transformer encoder \(\mathrm{Transf}(\cdot)\) via three differentiable modes:

\paragraph{Prefix‐Token Injection}

Prepend \(s\) as a synthetic token:
\[
H' = [\,s;\,h_1;\dots;h_T\,]\in\mathbb{R}^{(T+1)\times d},
\]
with position embedding for the prefix. This enables global access from layer 0 \cite{li2021prefix}.

\paragraph{Attention‐Bias Injection}

Bias queries and keys:
\[
Q_i \leftarrow Q_i + W_Q\,s,
\quad
K_j \leftarrow K_j + W_K\,s,
\]
so attention scores become:
\[
A_{ij} = \exp\bigl((Q_i + W_Q s)^\top(K_j + W_K s)\bigr).
\]
Here \(W_Q,W_K\in\mathbb{R}^{d\times d}\) control how \(s\) shifts attention geometry \cite{hu2021lora}.

\paragraph{Gated‐FFN Injection}

Modulate feed-forward response:
\[
g = \sigma(W_g\,s + b_g)\in(0,1)^d,
\quad
h'_i = h_i + g\odot\mathrm{FFN}(h_i),
\]
where \(\odot\) denotes elementwise multiplication. This adaptively gates nonlinear transformation based on span context \cite{shazeer2017outrageously}.

\subsubsection{Multi-Path Fusion}

We learn nonnegative scalar weights \(\alpha,\beta,\gamma\) to interpolate injection modes:
\[
\mathrm{Transf}\bigl(
\alpha\,\textsc{(Prefix)} + 
\beta\,\textsc{(Attn-Bias)} +
\gamma\,\textsc{(Gated-FFN)}
\bigr),
\]
preserving full differentiability and allowing dynamic tuning of injection strategy.