\subsection{Runtime Complexity}
\label{sec:runtime-complexity}

We analyze the computational cost of a single forward pass in X-Spanformer by decomposing it into six stages. Let
\[
T = \text{input length (codepoints)},\quad
V = \text{vocabulary size},\quad
d = \text{hidden dimension},\quad
w_{\max} = \text{maximum span width},\quad
K = \text{number of retained spans},\quad
d_f = \text{MLP hidden dim},\quad
\eta \in \{0,1\} = \text{prefix token flag}.
\]
We assume \(V=O(10^3)\), \(d_f = O(d)\), \(K\ll T\), and \(w_{\max}\ll T\).

\subsubsection{Soft Segmentation and Seed Embedding}

A custom Unigram-LM operator (Sec.~\ref{sec:vocab-induction}) computes a sparse probability matrix \(P\in\mathbb{R}^{T\times V}\), then multiplies by the embedding table \(W_{\mathrm{emb}}\in\mathbb{R}^{V\times d}\):
\[
P: O(TV), 
\qquad
H^0 = P\,W_{\mathrm{emb}}: O(TVd).
\]
In practice, since \(V\approx10^3\), this step costs \(O(Td)\). The result \(H^0\in\mathbb{R}^{T\times d}\) forms the seed embeddings.

\subsubsection{Contextual Encoder}

To imbue local context without quadratic overhead, \(H^0\) is passed through a convolutional encoder (Sec.~\ref{sec:seed-embeddings}):
\[
H = \mathrm{ConvNet}(H^0): O(Td^2).
\]
This encoder uses stacked or dilated 1D convolutions, capturing fixed-window dependencies efficiently.

\subsubsection{Span Enumeration and Boundary Scoring}

For each position \(i\), up to \(w_{\max}\) spans are scored via two linear heads on \(H\):
\[
\ell^s = W_s H + b_s,\quad
\ell^e = W_e H + b_e,\quad
\mathrm{score}(i,j)=p^s_i\,p^e_j,
\]
requiring \(O(Td)\) for head projections and \(O(Tw_{\max})\) for score computation, totaling \(O(Td + T\,w_{\max})\).

\subsubsection{Span Embedding and Scoring}

The top-\(K\) spans are pooled and optionally refined by self-attention:
\[
\mathrm{Pool}: O(K\,w_{\max}\,d), 
\qquad
\mathrm{SelfAttn}: O(K\,w_{\max}^2\,d).
\]
Relevance logits are produced by an MLP of size \(d_f\):
\[
\mathrm{MLP}: O(K\,d_f), 
\quad
\mathrm{softmax}: O(K), 
\quad
\mathrm{fusion}: O(K\,d).
\]
Since \(d_f = O(d)\), the combined cost is \(O(Kd)\).

\subsubsection{Controller Injection}

Computing the fused controller \(s=\sum_k a_k s_k\) costs \(O(Kd)\). Injecting \(s\) into the transformer (via bias, gating, or prefix) adds \(O(Td)\), which is dominated by the final contextualization.

\subsubsection{Joint Contextualization}

A full transformer layer over \(T+\eta\) tokens incurs:
\[
O\bigl((T+\eta)^2\,d\bigr).
\]

\begin{proposition}[Asymptotic Complexity of One Forward Pass]
	Under the above notation, X-Spanformerâ€™s per-example time is
	\[
	O\bigl(TVd + Td^2 + T\,w_{\max} + K\,d + (T+\eta)^2\,d\bigr),
	\]
	which, with \(V,\eta,d\) treated as constants, simplifies to
	\[
	O\bigl(T^2\,d + T\,w_{\max} + K\,d\bigr).
	\]
\end{proposition}

\begin{proof}
	Summing the cost of each stage:
	\[
	O(TVd) + O(Td^2) + O(Td + T\,w_{\max}) + O(Kd) + O(Kd) + O(Td) + O((T+\eta)^2d).
	\]
	Dropping lower-order and absorbed terms yields
	\[
	O(T^2\,d + T\,w_{\max} + K\,d).
	\]
\end{proof}

This decomposition mirrors that of sparse-attention and routing Transformers such as Longformer \cite{beltagy2020longformer}, BigBird \cite{zaheer2020bigbird}, and MoE layers \cite{shazeer2017outrageously,ainslie2023transformers}. The segmentation and fusion stages scale subquadratically in \(T\), confining the quadratic bound only to the final encoder layer.