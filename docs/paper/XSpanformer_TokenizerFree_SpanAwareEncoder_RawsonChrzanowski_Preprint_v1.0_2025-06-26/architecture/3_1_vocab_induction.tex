\subsection{Hybrid Unigram‐LM Vocabulary Induction}
\label{sec:vocab-induction}

We construct a data‐adaptive subword vocabulary by training a Unigram language model (LM) over all substrings up to length \(L_{\max}\), then pruning pieces based on both corpus perplexity and coverage (OOV rate). Let
\(\mathcal X = \{x^{(i)}\}_{i=1}^N\)
be our corpus of raw Unicode codepoint sequences of total length \(T\), and let \(\mathcal U_{0}\) denote the initial candidate set formed by extracting every substring of length \(\le L_{\max}\), retaining the top \(M\) by frequency, and including all individual codepoints.

\subsubsection{Candidate Generation}

For each sequence \(x \in \mathcal X\), we slide a window of width \(\le L_{\max}\) across all codepoints to extract substrings. We record their frequency and retain the top \(M\) most frequent substrings, subject to whitespace coherence constraints (detailed in Section~\ref{sec:whitespace-tokenization}). The resulting candidate set is:
\[
\mathcal U_{0}
= \{\text{top }M\text{ valid substrings}\} \cup \{\text{all single codepoints}\},
\]
ensuring base coverage, bounding vocabulary size, and maintaining strict separation between whitespace and content tokens.

\subsubsection{Whitespace-Aware Tokenization}
\label{sec:whitespace-tokenization}

Traditional subword tokenizers often fragment whitespace sequences or create suboptimal splits that break semantic boundaries, particularly problematic for code and mixed natural language-code content where spatial structure carries meaning. We address this through a strict separation principle: \textbf{whitespace sequences are always standalone atomic tokens}.

\paragraph{Strict Whitespace Separation}

We enforce that whitespace and non-whitespace characters never mix within a single token, using Python's standard \texttt{string.whitespace} definition that includes all Unicode whitespace control characters: \texttt{\{' ', '\textbackslash t', '\textbackslash n', '\textbackslash r', '\textbackslash x0b', '\textbackslash x0c'\}}. This creates two distinct classes of valid candidates:

\begin{align}
\mathcal{W} &= \{\underbrace{\text{ws} \cdots \text{ws}}_{k \text{ times}} \mid \text{ws} \in \mathcal{C}_w, k \geq 1\} \quad\text{(pure whitespace)} \\
\mathcal{N} &= \{\text{sequences containing no characters from } \mathcal{C}_w\} \quad\text{(pure non-whitespace)}
\end{align}

where $\mathcal{C}_w = \{\text{SPACE}, \text{TAB}, \text{LF}, \text{CR}, \text{VT}, \text{FF}\}$ represents all standard whitespace control characters.

The valid candidate constraint becomes:
\[
u \in \mathcal{U}_{\text{valid}} \iff u \in \mathcal{W} \cup \mathcal{N}
\]

This strictly prohibits mixed tokens such as \texttt{" the"}, \texttt{"ing "}, \texttt{"a\textbackslash tb"}, \texttt{"\textbackslash nhello"}, ensuring clean separation between content and all whitespace characters.

\paragraph{Atomic Whitespace Sequences}

Whitespace sequences form natural tokens through frequency-based selection, encompassing all standard control characters:

\begin{itemize}
    \item Spacing: \texttt{" "}, \texttt{"  "}, \texttt{"    "} (repeated spaces)
    \item Indentation: \texttt{"\textbackslash t"}, \texttt{"\textbackslash t\textbackslash t"} (horizontal tabs)
    \item Line control: \texttt{"\textbackslash n"}, \texttt{"\textbackslash r"}, \texttt{"\textbackslash n\textbackslash n"}, \texttt{"\textbackslash r\textbackslash n"} (line feeds, carriage returns)
    \item Form control: \texttt{"\textbackslash x0b"}, \texttt{"\textbackslash x0c"} (vertical tab, form feed)
    \item Mixed sequences: \texttt{"\textbackslash t "}, \texttt{" \textbackslash n"}, \texttt{"\textbackslash n  "} (realistic whitespace combinations)
\end{itemize}

For whitespace sequences exceeding the length limit $L_{\max}$ or not present in the learned vocabulary, Viterbi segmentation naturally decomposes them into multiple smaller whitespace tokens. For example, a 12-space indentation might segment as $[\texttt{"    "}, \texttt{"    "}, \texttt{"    "}]$ if 4-space tokens are frequent, or $[\texttt{"  "}, \texttt{"  "}, \texttt{"  "}, \texttt{"  "}, \texttt{"  "}, \texttt{"  "}]$ if 2-space tokens are more common in the corpus.

\paragraph{Consistent Tokenization}

This approach ensures that text like \texttt{"function(x, y)"} is consistently tokenized as separate content and spacing units:
\[
\text{Example: } \texttt{"function(x,\ y)"} \rightarrow [\texttt{"function"}, \texttt{"("}, \texttt{"x"}, \texttt{","}, \texttt{" "}, \texttt{"y"}, \texttt{")"}]
\]
where single space characters are atomic tokens, maintaining clear boundaries between semantic content and structural spacing.

\subsubsection{Unigram LM via EM}

We initialize the piece probabilities by normalizing raw frequency counts:
\[
p^{(0)}(u)
= \frac{\mathrm{freq}(u)}{\sum_{v\in\mathcal U_{0}}\mathrm{freq}(v)},
\quad
\forall\,u\in\mathcal U_{0}.
\]

At each EM iteration \(t\), we alternate:

\paragraph{E-step:}

For each sequence \(x\in\mathcal X\), we compute the posterior usage mass \(\gamma^{(t)}(u \mid x)\) for every piece \(u\in \mathcal U_t\). Since enumerating all segmentations of \(x\) is intractable, we approximate using the single best segmentation:
\[
\mathrm{seg}^{*}(x)
= \arg\max_{\mathrm{seg}} \prod_{v\in\mathrm{seg}} p^{(t)}(v),
\]
obtained via Viterbi decoding \cite{kudo2018sentencepiece}. We then compute:
\[
\gamma^{(t)}(u \mid x)
= \sum_{v \in \mathrm{seg}^{*}(x)} \mathbf{1}_{u = v}.
\]

\paragraph{M-step:}

We re-estimate the piece probabilities as:
\[
p^{(t+1)}(u)
= \frac{
	\sum_{x \in \mathcal X} \gamma^{(t)}(u \mid x)
}{
	\sum_{v \in \mathcal U_t} \sum_{x \in \mathcal X} \gamma^{(t)}(v \mid x)
},
\]
which reduces to relative frequency over the Viterbi segmentations.

\subsubsection{Baseline Perplexity}

To benchmark compressibility before pruning, we compute the initial corpus perplexity using piece-level normalization for consistency with the pruning criterion:
\[
\mathrm{PPL}^{(0)}
= \exp\left(
\frac{L^{(0)}}{N_{p}^{(0)}}
\right),
\]
where:
\[
L^{(0)} = -\sum_{x\in\mathcal X}\;\sum_{v\in\mathrm{seg}^{*}(x)}\log p^{(0)}(v)
\quad\text{(negative log-likelihood of all pieces)},
\]
\[
N_{p}^{(0)} = \sum_{x\in\mathcal X} \bigl|\mathrm{seg}^{*}(x)\bigr|
\quad\text{(total number of pieces)},
\]
and \(\mathrm{seg}^{*}(x)\) is the Viterbi segmentation under \(p^{(0)}\).

\paragraph{Mathematical Consistency Note:} 
This piece-level normalization ensures that the baseline perplexity \(\mathrm{PPL}^{(0)}\) and pruning perplexity \(\mathrm{PPL}'\) use the same normalization scheme, enabling meaningful comparison in the pruning criterion \(\mathrm{PPL}' - \mathrm{PPL}^{(t)} < \tau_{\mathrm{ppl}}\). An alternative sequence-level baseline would divide by \(|\mathcal X|\) instead of \(N_{p}^{(0)}\), but this would make the pruning comparison mathematically invalid since \(\mathrm{PPL}'\) uses piece-level normalization.







\subsubsection{Adaptive Pruning by PPL and OOV}

After each M‐step, we consider pruning any piece \(u\in V\) with \(p^{(t)}(u)<\epsilon\).  For a candidate removal \(V' = V \setminus \{u\}\), we first perform Viterbi segmentation under the reduced vocabulary:

\[
\mathrm{seg}^{*}_{V'}(x) \;=\;\arg\max_{\mathrm{seg}} \;\prod_{v\in\mathrm{seg}} p^{(t)}_{V'}(v)
\quad\text{for each }x\in\mathcal X.
\]

We then introduce the following quantities:

\[
N_{p}' \;=\;\sum_{x\in\mathcal X} \bigl|\mathrm{seg}^{*}_{V'}(x)\bigr|
\quad\text{(total number of subword pieces)},
\]

\[
L' \;=\; -\,\sum_{x\in\mathcal X}\;\sum_{v\in\mathrm{seg}^{*}_{V'}(x)}\log p^{(t)}_{V'}(v)
\quad\text{(negative log‐likelihood of all pieces)},
\]

\[
N_{t} \;=\;\sum_{x\in\mathcal X} |x|
\quad\text{(total number of codepoint tokens)},
\]

\[
N_{\mathrm{uncov}}' 
\;=\; \sum_{x\in\mathcal X}\;\sum_{i=1}^{|x|}
\mathbf1\bigl(i\not\in \mathrm{cover}_{V'}(x)\bigr)
\quad\text{(total uncovered positions)},
\]

where \(\mathrm{cover}_{V'}(x)\) is the set of codepoint indices spanned by \(\mathrm{seg}^{*}_{V'}(x)\).

Using these, we define:

\[
\mathrm{PPL}' 
= \exp\Bigl(\tfrac{L'}{N_{p}'}\Bigr),
\quad
\mathrm{OOV}' 
= \frac{N_{\mathrm{uncov}}'}{N_{t}}.
\]

We accept the removal \(u\) if and only if both
\[
\mathrm{PPL}' - \mathrm{PPL}^{(t)} < \tau_{\mathrm{ppl}}
\quad\text{and}\quad
\mathrm{OOV}' \le \delta_{\mathrm{oov}}.
\]

This criterion guarantees that any pruning step degrades the model’s average piece‐level log‐likelihood by at most \(\tau_{\mathrm{ppl}}\) and introduces no more than \(\delta_{\mathrm{oov}}\) uncovered codepoint positions.









\subsubsection{Existence and Monotonicity Proposition}

Define the feasible set:
\[
\mathcal F(\tau,\delta)
= \left\{\,V\subseteq\mathcal U_{0}\,\middle|\,
\mathrm{PPL}(V)\le \mathrm{PPL}^{(0)}+\tau,\;
\mathrm{OOV}(V)\le \delta
\right\}.
\]

\begin{proposition}[Feasibility and Monotonicity]
	For any \(\tau,\delta \ge 0\), the feasible set \(\mathcal F(\tau,\delta)\) is nonempty. Moreover, if \(\tau' \ge \tau\), \(\delta' \ge \delta\), then:
	\[
	\mathcal F(\tau,\delta) \subseteq \mathcal F(\tau',\delta'),
	\quad
	\Rightarrow
	\quad
	\min_{V\in\mathcal F(\tau',\delta')}|V| \le \min_{V\in\mathcal F(\tau,\delta)}|V|.
	\]
\end{proposition}

\begin{proof}
	\textbf{Step 1: Existence}
	
	Choose \(V = \mathcal U_0\), the full candidate set.
	
	Since \(V\) includes all substrings and individual codepoints:
	\[
	\mathrm{OOV}(V) = 0,
	\quad
	\mathrm{PPL}(V) = \mathrm{PPL}^{(0)}.
	\]
	
	Therefore, for any \(\tau,\delta \ge 0\),
	\[
	\mathrm{PPL}(V) \le \mathrm{PPL}^{(0)} + \tau,
	\quad
	\mathrm{OOV}(V) \le \delta,
	\quad
	\Rightarrow
	V \in \mathcal F(\tau,\delta).
	\]
	
	\textbf{Step 2: Monotonicity}
	
	Let \(V \in \mathcal F(\tau,\delta)\), and suppose \(\tau' \ge \tau\), \(\delta' \ge \delta\). Then:
	\[
	\mathrm{PPL}(V) \le \mathrm{PPL}^{(0)} + \tau \le \mathrm{PPL}^{(0)} + \tau',
	\quad
	\mathrm{OOV}(V) \le \delta \le \delta',
	\quad
	\Rightarrow
	V \in \mathcal F(\tau',\delta').
	\]
	
	Thus:
	\[
	\mathcal F(\tau,\delta) \subseteq \mathcal F(\tau',\delta').
	\]
	
	\textbf{Step 3: Minimality}
	
	Since \(\mathcal F(\tau,\delta) \subseteq \mathcal F(\tau',\delta')\),
	\[
	\min_{V \in \mathcal F(\tau',\delta')} |V| \le \min_{V \in \mathcal F(\tau,\delta)} |V|.
	\]
\end{proof}

\subsubsection{Algorithm: Adaptive Vocabulary Induction}

To construct an efficient and corpus-aware subword vocabulary, we apply an expectation-maximization loop over the candidate set \(\mathcal{U}_0\), guided by a Viterbi-decoded likelihood objective. Starting from raw frequency estimates, we refine piece probabilities \(p(u)\) via best-segmentation token counts and normalize them across the corpus. After each M-step, we attempt to prune low-probability pieces while preserving compressibility and coverage. Each candidate removal is simulated by resegmenting \(\mathcal{X}\) under the reduced vocabulary \(V'\) and evaluating both perplexity and position-level OOV rate. If the degradation in log-likelihood is bounded and no coverage gaps are introduced, the removal is accepted. This selective pruning yields a compact, entropy-regularized vocabulary tailored to the domain structure.

\begin{algorithm}[H]
	\caption{Adaptive Unigram‐LM Vocabulary Induction}
	\label{alg:vocab-induction}
	\begin{algorithmic}[1]
		\STATE Extract candidate substrings up to length \(L_{\max}\) from corpus \(\mathcal{X}\); form initial vocabulary \(\mathcal{U}_0\)
		\STATE Initialize piece probabilities: \(p^{(0)}(u) \propto \mathrm{freq}(u)\) for all \(u \in \mathcal{U}_0\)
		\STATE Compute baseline perplexity \(\mathrm{PPL}^{(0)} = \exp(L^{(0)}/N_p^{(0)})\) via Viterbi decoding over \(\mathcal{X}\)
		\FOR{iteration \(t = 0\) to \(T_{\max}\)}
		\FOR{each sequence \(x \in \mathcal{X}\)}
		\STATE Compute best segmentation \(\mathrm{seg}^{*}(x)\) using current \(p^{(t)}\)
		\STATE Accumulate token usage counts \(\gamma^{(t)}(u \mid x)\) for all \(u \in \mathrm{seg}^{*}(x)\)
		\ENDFOR
		\STATE Update probabilities: normalize counts to get \(p^{(t+1)}(u)\)
		\STATE Set current vocabulary \(V = \{u \mid p^{(t+1)}(u) > 0\}\)
		\FOR{each \(u \in V\) with \(p^{(t+1)}(u) < \epsilon\)}
		\STATE Tentatively prune: define \(V' = V \setminus \{u\}\)
		\FOR{each sequence \(x \in \mathcal{X}\)}
		\STATE Decode \(\mathrm{seg}^{*}_{V'}(x)\) using updated \(V'\)
		\STATE Compute log-prob score and uncovered positions
		\ENDFOR
		\STATE Compute \(\mathrm{PPL}'\) and \(\mathrm{OOV}'\) from new segmentations
		\IF{\(\mathrm{PPL}' - \mathrm{PPL}^{(t)} < \tau_{\mathrm{ppl}}\) and \(\mathrm{OOV}' \le \delta_{\mathrm{oov}}\)}
		\STATE Accept removal: \(V \leftarrow V'\)
		\ENDIF
		\ENDFOR
		\STATE Update \(\mathrm{PPL}^{(t+1)}\) using accepted vocabulary \(V\)
		\ENDFOR
		\STATE \textbf{Return} final pruned vocabulary \(V\) and piece probabilities \(\{p(u)\}\)
	\end{algorithmic}
\end{algorithm}