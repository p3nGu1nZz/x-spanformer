\section{Experiments}
\label{sec:experiments}

In this section, we analyze the emergent behavior and structural control capacity of the proposed X-Spanformer architecture through a series of controlled experiments. Our objectives are threefold:
\begin{enumerate}[leftmargin=2em]
    \item To verify that differentiable span selection converges toward semantically meaningful structures under entropy annealing;
    \item To evaluate the fidelity and variance of controller vector injection across multiple integration pathways;
    \item To probe the interpretability and stability of span routing under synthetically constructed and naturalistic corpora.
\end{enumerate}

Unlike traditional benchmark-driven evaluations, our methodology emphasizes structural diagnostics and interpretability over end-task performance. This is consistent with experimental paradigms in latent structure induction \cite{kim2019unsupervised, naradowsky2021structured, ma2023hierarchical}, probing analysis \cite{belinkov2022probing, hewitt2019structural}, and entropy-regularized representation learning \cite{pereyra2017regularizing, grandvalet2005semi}.

\vspace{0.5em}
\noindent We denote:
\begin{itemize}[leftmargin=1.6em]
  \item \(\mathcal{D} = \{(x^{(i)}, y^{(i)})\}_{i=1}^{N}\): training corpus with optional supervision;
  \item \(f_\theta\): differentiable span scorer;
  \item \(g_\phi\): controller aggregator;
  \item \(\tilde{s}\): controller vector, computed as a relevance-weighted sum over pooled span embeddings. The construction proceeds in two steps:
  
  \emph{Step 1: Relevance Score Computation}
  For each selected span \(k \in \{1,\ldots,K\}\), we compute a relevance score \(w_k\) based on the span embedding \(s_k\), span length \(\delta_k\), and confidence measure \(\mathrm{conf}_k\):
  \begin{equation}
  w_k = g_\phi(s_k, \delta_k, \mathrm{conf}_k)
  \end{equation}
  where \(g_\phi\) is a learned aggregator network parameterized by \(\phi\).
  
  \emph{Step 2: Softmax Normalization and Weighted Sum}
  The relevance scores are normalized via softmax to obtain attention weights:
  \begin{equation}
  \alpha_k = \frac{\exp(w_k)}{\sum_{\ell=1}^{K} \exp(w_\ell)}, \quad k = 1,\ldots,K
  \label{eq:attention_weights}
  \end{equation}
  
  Note that \(\sum_{k=1}^K \alpha_k = 1\) and \(\alpha_k \ge 0\) for all \(k\), forming a probability distribution over spans.
  
  The final controller vector is computed as:
  \begin{equation}
  \tilde{s} = \sum_{k=1}^{K} \alpha_k s_k
  \label{eq:controller_vector}
  \end{equation}
  
  This construction ensures that \(\tilde{s}\) lies in the convex hull of the span embeddings \(\{s_k\}_{k=1}^K\).
  \item \(\psi\): transformer parameters.
\end{itemize}

Model optimization proceeds via the composite loss:
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \beta_1 \mathcal{L}_{\text{span}} + \beta_2 \mathcal{L}_{\text{ent}},
\label{eq:exp_loss_summary}
\end{equation}
where:
\begin{itemize}[leftmargin=1.8em]
    \item \(\mathcal{L}_{\text{task}}\): task-aligned objective (e.g., cross-entropy, contrastive alignment);
    \item \(\mathcal{L}_{\text{span}}\): span KL alignment term, which encourages the learned span distribution \(P\) to match supervised gold spans \(\hat{P}_{\text{gold}}\). This is formulated as the Kullback-Leibler divergence:
    \begin{equation}
    \mathcal{L}_{\text{span}} = \mathrm{KL}(\hat{P}_{\text{gold}} \,\|\, P) = \sum_{(i,j)} \hat{P}_{\text{gold}}(i,j) \log \frac{\hat{P}_{\text{gold}}(i,j)}{P(i,j)}
    \label{eq:kl_span}
    \end{equation}
    where the sum is over all valid span positions \((i,j)\) with \(1 \le i < j \le T\). This term is non-negative and equals zero if and only if \(P = \hat{P}_{\text{gold}}\) almost everywhere.
    \item \(\mathcal{L}_{\text{ent}} = - \lambda_{\text{ent}} H(P)\): entropy regularization term.
\end{itemize}

To isolate structural behavior, we evaluate:
\begin{itemize}
  \item Span distribution entropy \(H(P) = -\sum_{(i,j)} P_{ij} \log P_{ij}\);
  \item Controller gate variance \(\mathrm{Var}(\sigma(W_g \tilde{s}))\);
  \item Span overlap rate: fraction of selected spans sharing token positions;
  \item Downstream impact: change in token-level logit outputs under controller ablation.
\end{itemize}

\vspace{0.5em}
\noindent\textbf{Experimental Philosophy.}
Our experiments are structured not as competitive benchmarks, but as architectural diagnostics to validate the inductive mechanism of span-aware routing. This aligns with prior work in structural probing and latent routing models \cite{gupta2022molt, tay2020sparse, clark2018semi}.

\vspace{0.75em}
\textbf{Note:} All results in this section are presented for illustrative and developmental purposes. Empirical benchmarks for generalization, transferability, and performance scaling are left to future work as model weights stabilize and structure supervision matures.

\input{sections/experiments/5_1_experimental_setup}
\input{sections/experiments/5_2_span_routing_behavior}
\input{sections/experiments/5_3_controller_fusion_diagnostics}
\input{sections/experiments/5_4_qualitative_span_interpretability}
\input{sections/experiments/5_5_ablation_studies}
\input{sections/experiments/5_6_future_benchmarks}
