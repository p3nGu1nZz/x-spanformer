\section{Training}

X-Spanformer is trained end-to-end to jointly learn both span induction and controller fusion.  Denote by 
\(\mathcal{D}=\{(x^{(i)},y^{(i)})\}_{i=1}^N\) 
the training corpus, where each input \(x^{(i)}\) is a sequence of \(T\) Unicode codepoints encoded into seed embeddings \(H^0\in\mathbb{R}^{T\times d}\) (Section 3.1).  Let \(H=\mathrm{Encoder}(H^0)\in\mathbb{R}^{T\times d}\) be the contextualized representations.  The model optimizes a composite loss

\[
\mathcal{L}_{\mathrm{total}}
= \mathcal{L}_{\mathrm{task}}
+ \beta_1\,\mathcal{L}_{\mathrm{span}}
+ \beta_2\,\mathcal{L}_{\mathrm{ent}},
\]

where
\(\mathcal{L}_{\mathrm{task}}\) is a task‐specific objective (e.g.\ cross‐entropy or contrastive loss),
\(\mathcal{L}_{\mathrm{span}}\) encourages alignment to any available span supervision, and
\(\mathcal{L}_{\mathrm{ent}}\) is an entropy‐based regularizer that drives early exploration.

The training pipeline consists of three fully differentiable stages:
\begin{itemize}
	\item \textbf{Span induction with entropy‐regularized scoring}: generate and score candidate spans via a differentiable pointer network (Section 3.2), regularized to maintain high entropy early on.
	\item \textbf{Relevance‐weighted fusion}: pool and encode the top-\(K\) spans, compute relevance logits, and interpolate into a controller vector \(s\) (Section 3.5–3.7).
	\item \textbf{Controller‐aware injection}: condition the transformer backbone via prefix insertion, attention‐bias shifts, or gated FFN (Section 3.6).
\end{itemize}

\input{sections/training/4_1_span_induction}
\input{sections/training/4_2_controller_injection}
\input{sections/training/4_3_end_to_end_finetuning}