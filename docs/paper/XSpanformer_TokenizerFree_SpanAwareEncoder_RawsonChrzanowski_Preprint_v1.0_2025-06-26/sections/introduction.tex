\section{Introduction}

Transformer architectures have become the foundation for advances in natural language understanding, program synthesis, and multimodal retrieval \cite{vaswani2017attention,devlin2019bert,radford2019gpt2,raffel2020t5}.  A critical precondition for these models is the segmentation of raw text into discrete units—most commonly fixed subwords produced by Byte‐Pair Encoding (BPE) \cite{sennrich2016bpe} or SentencePiece \cite{kudo2018sentencepiece}.  While such static vocabularies yield efficient lookup and in‐domain accuracy, they impose irrevocable lexical boundaries that (i) degrade under domain shift \cite{galle2021respite}, (ii) obscure long‐range compositional patterns in code and multilingual text, and (iii) demand costly re‐training or vocabulary expansion to accommodate novel syntax or semantics.

Segmentation is traditionally decoupled from model training, treated as an irreversible preprocessing step that precludes gradient flow and adaptation to downstream objectives.  Recent work in character‐aware encoding and tokenization‐free models—such as Charformer \cite{tay2021charformer}, CANINE \cite{clark2021canine}, and probabilistically masked language models \cite{liu2022learnedsegmentation,liu2022pmlm}—demonstrates the potential of integrating segmentation into end‐to‐end learning.  However, these methods either omit explicit linguistic priors or fail to produce interpretable, overlapping segments aligned with phrase‐level semantics.

Drawing on the X‐bar schema from generative grammar \cite{jackendoff1977xbar}, we introduce X-Spanformer, a span‐based segmentation module that is fully differentiable and ONNX‐native.  X-Spanformer begins with a hybrid Unigram‐LM front‐end—combining all UTF-8 characters with top entropy‐pruned subword fragments via SentencePiece’s Unigram mode—to compute a soft probability matrix \(P\in\mathbb{R}^{T\times V}\) over raw Unicode codepoints.  A factorized pointer network \cite{vinyals2015pointer} then predicts variable‐length, overlapping span candidates, which are softly typed by modality, filtered by a learned length estimator, and pooled into \(d\)‐dimensional embeddings.  These embeddings are fused into a single controller vector \(s\) via relevance‐weighted interpolation and injected into downstream transformers through prefix, attention‐bias, or gated‐FFN pathways.  Because all components are differentiable, X-Spanformer supports end‐to‐end optimization of segmentation and task objectives, while maintaining compressor efficiency and interpretability.

\subsection{Contributions}

This paper makes the following contributions:
\begin{enumerate}
	\item We formalize tokenizer‐free segmentation as a span‐prediction problem grounded in X‐bar theory, instantiated with a hybrid Unigram‐LM front‐end, dynamic span capping, and modality typing.
	\item We propose a multi‐phase curriculum that bootstraps span induction from synthetic BPE labels and transitions to entropy‐regularized and type‐aware supervision.
	\item We design an ONNX‐native architecture for compositional pooling and controller fusion, detailing prefix‐token, attention‐bias, and gated‐FFN injection modes.
	\item We introduce an evaluation framework—covering compression ratio, contrastive retrieval alignment, span entropy, and modality coherence—and release an ONNX‐compatible implementation with training recipes and corpus construction guidelines.
\end{enumerate}