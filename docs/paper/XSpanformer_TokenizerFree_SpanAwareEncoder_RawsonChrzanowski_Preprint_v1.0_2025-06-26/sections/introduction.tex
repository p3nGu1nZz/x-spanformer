\section{Introduction}

Transformer architectures underpin state‐of‐the‐art performance in natural language understanding, code generation, and multimodal retrieval \cite{vaswani2017attention,devlin2019bert,radford2019gpt2,raffel2020t5}.  A core prerequisite for these models is the segmentation of raw text or code into discrete units, most often fixed subwords derived from Byte‐Pair Encoding (BPE) \cite{sennrich2016bpe} or Unigram‐LM vocabularies \cite{kudo2018sentencepiece}.  Although static vocabularies enable efficient embedding lookup and in‐domain accuracy, they introduce rigid lexical boundaries that (i) degrade performance under domain shift \cite{galle2021respite}, (ii) obscure long‐range compositional patterns in code, multilingual, and hybrid text, and (iii) require costly re‐training or vocabulary expansion to handle novel syntax or semantics.  Moreover, treating segmentation as an irreversible preprocessing step prevents gradient flow and adaptation to downstream objectives.

To address these limitations, we propose X‐Spanformer, a tokenizer‐free, span‐aware segmentation module that is fully differentiable and ONNX‐native.  X‐Spanformer replaces static tokenization with an adaptive, corpus‐driven induction of a hybrid Unigram‐LM vocabulary via an expectation–maximization routine approximated by Viterbi decoding, followed by entropy‐ and perplexity‐driven pruning to control vocabulary size and coverage.  The resulting soft assignment matrix \(P\in\mathbb{R}^{T\times V}\) is projected into seed embeddings and contextualized by a lightweight convolutional encoder that scales linearly in sequence length.  A factorized pointer network locates overlapping, variable‐length span candidates, which are softly typed by modality entropy, filtered by a learned length estimator, and pooled into \(d\)‐dimensional span embeddings.  We fuse the top‐\(K\) spans into a single controller vector via relevance‐weighted interpolation and inject it into downstream transformers through bias, prefix, or gated‐FFN mechanisms.  Because every component is differentiable and ONNX‐compatible, X‐Spanformer supports end‐to‐end training with an entropy‐regularized span induction curriculum, yielding interpretable segmentation aligned with phrase‐level structure and efficient inference across domains.

\subsection{Contributions}

This paper makes the following contributions:
\begin{enumerate}
	\item We formalize tokenizer‐free segmentation as a span‐prediction problem grounded in X‐bar theory, driven by a hybrid Unigram‐LM front end with Viterbi‐based EM and adaptive perplexity/OOV pruning.
	\item We introduce a convolutional contextual encoder that augments seed embeddings with local structure at linear cost, prioritizing throughput and interpretability over global self‐attention.
	\item We design an ONNX‐native architecture for span pooling, modality‐aware scoring, and controller fusion, detailing multiple injection pathways (prefix, bias, gated‐FFN) for flexible downstream integration.
	\item We develop an entropy‐regularized, multi‐phase curriculum that bootstraps from synthetic BPE supervision to type‐aware signals, enabling robust span induction across code, language, and hybrid modalities.
	\item We propose a comprehensive evaluation suite—including compression ratio, contrastive retrieval alignment, span entropy, modality coherence, and inference latency—and release an open‐source ONNX implementation with training recipes and corpus construction guidelines.
\end{enumerate}