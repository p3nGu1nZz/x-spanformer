\section{Introduction}

Transformer architectures underpin leading solutions in natural language understanding, program synthesis, and multimodal retrieval \cite{vaswani2017attention, devlin2019bert, radford2019gpt2, raffel2020t5}. Central to these models is a static segmentation stage that partitions input into fixed subword units, most commonly via Byte‐Pair Encoding \cite{sennrich2016bpe} or SentencePiece \cite{kudo2018sentencepiece}. While effective on in‐domain corpora, these pipelines impose immutable lexical boundaries that degrade under domain shift, obscure long‐range compositional patterns in code and multilingual text \cite{galle2021respite}, and incur substantial costs when vocabularies must be revised for novel syntactic or semantic phenomena.

Segmentation is traditionally decoupled from model training, treated as an irreversible preprocessing operation that lacks gradient flow and cannot adapt to downstream objectives. Recent work in character‐aware encoding \cite{taylor2021charformer}, tokenization‐free models \cite{clark2021canine}, and unsupervised segmentation in sequential domains \cite{liu2022learnedsegmentation, liu2022pmlm} demonstrates the potential of adaptive boundaries. However, these approaches often omit linguistic structure and do not offer interpretable segmentation aligned with phrase‐level semantics.

Drawing on the X‐bar schema from generative grammar \cite{jackendoff1977xbar}, we posit that raw token streams (for example, source code, natural language, or symbolic hybrids) exhibit latent hierarchical units that can be learned directly from data. We introduce \textbf{X-Spanformer}, a span-based segmenter that formulates boundary detection as a pointer-network prediction task \cite{vinyals2015pointer}. Beginning with a compact one-thousand-unit BPE seed, X-Spanformer learns to emit overlapping, variable-length spans that are softly typed by modality (for example, code, natural language, or identifier) and capped per sequence via a learned length estimator. Span representations are aggregated by pooling and integrated into downstream transformer encoders, enabling joint optimization of segmentation and task-specific objectives.

\subsection{Contributions}

This paper presents the following contributions:
\begin{enumerate}
  \item A formalization of tokenizer-free segmentation as a span-prediction problem grounded in X-bar theory, instantiated with a pointer network featuring dynamic span capping and modality typing.
  \item A curriculum learning paradigm that bootstraps span discovery from synthetic BPE labels and progressively shifts to contrastive and type-aware supervision.
  \item Architectural guidelines for embedding the span predictor into transformer encoders through compositional pooling and minimal layer extensions.
  \item A proposed evaluation framework covering compression ratio, contrastive alignment, span entropy analysis, and interpretability visualizations, accompanied by an ONNX-compatible implementation and complete training recipes.
\end{enumerate}