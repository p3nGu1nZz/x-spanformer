\section{Conclusion}

We have presented X-Spanformer, a fully differentiable, tokenizer-free segmentation front end that integrates variable-length, overlapping spans into transformer encoders.  Beginning with an EM-induced hybrid Unigram-LM vocabulary—approximated via Viterbi decoding and adaptively pruned on per-piece perplexity and OOV constraints—the model embeds soft piece assignments and contextualizes them with a linear-time convolutional encoder.  A factorized pointer network then proposes span candidates, which are filtered by learned length and modality priors, pooled into \(d\)-dimensional embeddings via gated self-attention, and fused into a single controller vector through relevance-weighted interpolation.  This vector is injected into downstream layers via prefix-token, attention-bias, or gated-FFN pathways, enabling end-to-end optimization under an entropy-regularized span induction curriculum.  We have provided precise mathematical formulations, algorithmic guarantees on vocabulary feasibility and pruning monotonicity, and an analysis showing subquadratic end-to-end complexity in sequence length.

An open-source ONNX-compatible implementation, complete with training recipes and corpus construction guidelines, has been released to facilitate reproducibility and benchmarking.  In future work, we will conduct extensive evaluations on cross-domain code understanding, multilingual parsing, and retrieval alignment to quantify improvements in compression efficiency, semantic coherence, and inference throughput.  We anticipate that X-Spanformer’s learned, grammar-inspired spans will surpass static tokenization in adaptability, interpretability, and downstream task performance, charting a path toward more structured and robust transformer architectures.