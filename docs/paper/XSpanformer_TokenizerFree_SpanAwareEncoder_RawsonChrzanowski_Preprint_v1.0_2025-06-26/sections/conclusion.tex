\section{Conclusion}

We have introduced X-Spanformer, a fully differentiable, tokenizer-free segmentation module that integrates learned spans into transformer encoders.  Starting from a hybrid Unigram-LM front-end, the model induces variable-length, overlapping spans via a factorized pointer network, regularizes them with learned length priors and entropy annealing, and enriches them with modality typing.  Span embeddings are composed through gated pooling and self-attention, fused into a single controller vector via relevance-weighted interpolation, and injected into the encoder through prefix-token, attention-bias, or gated-FFN pathways.  We’ve provided precise mathematical formulations for each component, theoretical guarantees on span selection and interpolation, and a two-phase curriculum that bootstraps from synthetic supervision to joint routing and representation learning—all while maintaining subquadratic preprocessing and end-to-end differentiability.

While formal evaluation is ongoing, we have released an ONNX-compatible implementation, training recipes, and corpus guidelines to facilitate community benchmarking.  Future work will rigorously assess X-Spanformer on tasks demanding robust segmentation—such as cross-domain code understanding, multilingual parsing, and retrieval alignment—to quantify gains in compression efficiency, semantic coherence, and downstream performance.  We anticipate that learned span induction will outperform static tokenization in adaptability and interpretability, paving the way for more structured and resilient transformer architectures.  