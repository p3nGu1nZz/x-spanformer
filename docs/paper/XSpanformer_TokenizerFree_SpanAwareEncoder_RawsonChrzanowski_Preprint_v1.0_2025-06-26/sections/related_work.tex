\section{Related Work}

\subsection{Static Subword Tokenization}
Most transformer pipelines rely on offline subword segmentation. Byte-Pair Encoding (BPE) constructs a fixed vocabulary by iteratively merging frequent symbol pairs extracted from a training corpus \cite{sennrich2016bpe}. SentencePiece builds on unigram language models to select subword tokens that maximize corpus likelihood \cite{kudo2018sentencepiece}. Such methods yield efficient lookup tables and have become ubiquitous in large-scale language models \cite{vaswani2017attention, devlin2019bert}, but they impose irrevocable boundaries that do not adapt during model training and can fragment semantically coherent units under domain shift \cite{galle2021respite}.

\subsection{Character-Level and Token-Free Models}
To mitigate subword brittleness, several works propose bypassing static vocabularies in favor of character- or byte-driven encoding. Charformer applies gradient-based subword tokenization to learn latent splits during pretraining, compressing sequences without a predefined vocabulary \cite{taylor2021charformer}. CANINE directly encodes Unicode codepoints with down-sampling and up-sampling layers, offering a tokenization-free encoder that matches BPE baselines on transfer tasks \cite{clark2021canine}. These approaches remove offline heuristics, but they do not explicitly model higher-order linguistic or symbolic structures.

\subsection{Unsupervised and Differentiable Segmentation}
Beyond character-level models, unsupervised segmentation methods aim to learn meaningful units directly from raw streams. Morfessor induces morpheme-like units via minimum description length objectives \cite{creutz2005unsupervised}. In the neural domain, probabilistically masked language models (PMLM) integrate segmentation into pretraining with masked span prediction \cite{liu2022pmlm}. Other works learn segmentation boundaries for text-to-text generation by optimizing a downstream reconstruction loss \cite{liu2022learnedsegmentation}. While these methods introduce differentiability, they lack explicit linguistic priors and produce non-overlapping, fixed partitions.

\subsection{Span-Based and Pointer-Network Models}
Pointer networks offer a mechanism for predicting variable-length spans by emitting start and end indices over an input sequence \cite{vinyals2015pointer}. SpanBERT extends this idea in pretraining by masking contiguous spans and predicting their content \cite{joshi2020spanbert}. In speech and vision, learned segmenters often output overlapping proposals that improve detection and alignment \cite{ren2015faster, zach2019segmenter}. However, to our knowledge no prior work unifies pointer-based span prediction with linguistically grounded structure for text segmentation in transformer encoders.

\subsection{Summary}

Existing segmentation strategies fall into three broad categories: offline subword tokenization, character‐level or token‐free encoders, and unsupervised boundary learners. Offline methods such as BPE and SentencePiece offer efficient lookups but impose static vocabularies that fragment long‐range structures and fail under domain shift \cite{sennrich2016bpe, kudo2018sentencepiece, galle2021respite}. Character‐level and byte‐level approaches eliminate heuristic preprocessing yet lack explicit modeling of phrase‐level regularities and often incur higher computational cost \cite{taylor2021charformer, clark2021canine}. Unsupervised segmentation methods introduce differentiability but produce non‐overlapping, monolithic partitions without linguistic priors \cite{creutz2005unsupervised, liu2022learnedsegmentation, liu2022pmlm}. Span‐based predictors and pointer‐network architectures enable variable‐length boundary proposals but have not been combined with generative grammar principles for text segmentation \cite{vinyals2015pointer, joshi2020spanbert}. X-Spanformer addresses these gaps by unifying pointer‐based span prediction with X-bar inspired inductive bias, yielding overlapping, softly typed spans that integrate seamlessly into transformer encoders and support end-to-end training.  