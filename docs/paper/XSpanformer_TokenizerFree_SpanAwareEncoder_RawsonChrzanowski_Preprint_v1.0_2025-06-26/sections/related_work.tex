\section{Related Work}

\subsection{Static Subword Tokenization}
Conventional transformer pipelines rely on fixed subword vocabularies, most prominently Byte‐Pair Encoding (BPE) \cite{sennrich2016bpe} and Unigram‐LM tokenizers such as SentencePiece \cite{kudo2018sentencepiece}. These methods facilitate efficient embedding lookup and strong in‐domain performance, but their immutable vocabularies cannot adapt during training, fragment compositional or rare phrases under domain shift \cite{galle2021respite}, and necessitate costly re‐tokenization when new syntax or terminology arise.

\subsection{Character- and Byte-Level Encoders}
To eliminate subword heuristics, character‐ and byte‐level models process raw codepoints directly. Charformer \cite{tay2021charformer} learns latent splits via gradient‐based tokenization, CANINE \cite{clark2021canine} applies down‐ and up‐sampling around raw Unicode, and ByT5 \cite{xue2022byt5} explores purely byte‐level inputs. While these approaches remove offline segmentation, they do not produce overlapping or hierarchical spans and rely on global self‐attention for context.

\subsection{Differentiable Segmentation}
Unsupervised and differentiable segmentation integrates boundary induction into neural pretraining. Morfessor \cite{creutz2005unsupervised} applies minimum description‐length to discover morphemes. Probabilistically Masked Language Models \cite{liu2022pmlm} and learned‐segmentation models \cite{liu2022learnedsegmentation} optimize reconstruction objectives to induce non‐overlapping partitions. These methods enable end‐to‐end learning but lack explicit modeling of overlapping spans or linguistic structure.

\subsection{Span-Based and Pointer Networks}
Pointer networks \cite{vinyals2015pointer} predict arbitrary start–end pairs to locate variable‐length spans. SpanBERT \cite{joshi2020spanbert} leverages this for masked span reconstruction, and similar architectures in vision and speech generate overlapping proposals for improved alignment \cite{ren2015faster,zach2019segmenter}. However, prior work has not combined pointer‐based, overlapping span induction with explicit grammar‐inspired priors within transformer encoders.

\subsection{Bridging Static and Dynamic Segmentation}
Hybrid vocabularies that merge character coverage with static subwords have been proposed to improve robustness \cite{tay2021charformer,clark2021canine}. Our approach extends this paradigm by training a hybrid Unigram‐LM front end—induced by an EM procedure approximated via Viterbi decoding and pruned on perplexity and OOV thresholds—implemented as an ONNX custom operator. We then employ a convolutional encoder to contextualize soft assignments, a factorized pointer network to propose overlapping spans, and a modality‐aware scoring mechanism to filter and fuse spans into a controller vector for downstream transformer injection.

\medskip
In contrast to purely static tokenizers, character‐level models, and non‐overlapping segmentation methods, X‐Spanformer unifies hybrid vocabulary induction, differentiable Viterbi‐based pruning, convolutional contextualization, and pointer‐network span prediction under a single, ONNX‐native framework. This design yields dynamic, interpretable spans that adapt to domain shifts and can be optimized end‐to‐end alongside downstream tasks.  