\subsection{Hybrid Unigram‐LM Vocabulary Induction}
\label{sec:vocab-induction}

We construct a data‐adaptive subword vocabulary by training a Unigram language model (LM) over all substrings up to length \(L_{\max}\), then pruning pieces based on both corpus perplexity and coverage (OOV rate). Let
\(\mathcal X = \{x^{(i)}\}_{i=1}^N\)
be our corpus of raw Unicode codepoint sequences of total length \(T\), and let \(\mathcal U_{0}\) denote the initial candidate set formed by extracting every substring of length \(\le L_{\max}\), retaining the top \(M\) by frequency, and including all individual codepoints.

\subsubsection{Candidate Generation}

For each sequence \(x \in \mathcal X\), we slide a window of width \(\le L_{\max}\) across all codepoints to extract substrings. We record their frequency and retain the top \(M\) most frequent substrings, subject to whitespace coherence constraints (detailed in Section~\ref{sec:whitespace-tokenization}). The resulting candidate set is:
\[
\mathcal U_{0}
= \{\text{top }M\text{ valid substrings}\} \cup \{\text{all single codepoints}\},
\]
ensuring base coverage, bounding vocabulary size, and maintaining strict separation between whitespace and content tokens.

\subsubsection{Whitespace-Aware Tokenization}
\label{sec:whitespace-tokenization}

Traditional subword tokenizers often fragment whitespace sequences or create suboptimal splits that break semantic boundaries, particularly problematic for code and mixed natural language-code content where spatial structure carries meaning. We address this through a strict separation principle: \textbf{whitespace sequences are always standalone atomic tokens}.

\paragraph{Strict Whitespace Separation}

We enforce that whitespace and non-whitespace characters never mix within a single token, using Python's standard \texttt{string.whitespace} definition\footnote{Python's \texttt{string.whitespace} includes all ASCII whitespace characters: space (\\u0020), tab (\\u0009), newline (\\u000a), vertical tab (\\u000b), form feed (\\u000c), and carriage return (\\u000d).} that includes all Unicode whitespace control characters: \texttt{\{' ', '\textbackslash t', '\textbackslash n', '\textbackslash r', '\textbackslash x0b', '\textbackslash x0c'\}}. This creates two distinct classes of valid candidates:

\begin{align}
\mathcal{W} &= \{\underbrace{\text{ws} \cdots \text{ws}}_{k \text{ times}} \mid \text{ws} \in \mathcal{C}_w, k \geq 1\} \quad\text{(pure whitespace)} \\
\mathcal{N} &= \{\text{sequences containing no characters from } \mathcal{C}_w\} \quad\text{(pure non-whitespace)}
\end{align}

where $\mathcal{C}_w = \{\text{SPACE}, \text{TAB}, \text{LF}, \text{CR}, \text{VT}, \text{FF}\}$ represents all standard whitespace control characters.

The valid candidate constraint becomes:
\[
u \in \mathcal{U}_{\text{valid}} \iff u \in \mathcal{W} \cup \mathcal{N}
\]

This strictly prohibits mixed tokens such as \texttt{" the"}, \texttt{"ing "}, \texttt{"a\textbackslash tb"}, \texttt{"\textbackslash nhello"}, ensuring clean separation between content and all whitespace characters.

\paragraph{Atomic Whitespace Sequences}

Whitespace sequences form natural tokens through frequency-based selection, encompassing all standard control characters:

\begin{itemize}
    \item Spacing: \texttt{" "}, \texttt{"  "}, \texttt{"    "} (repeated spaces)
    \item Indentation: \texttt{"\textbackslash t"}, \texttt{"\textbackslash t\textbackslash t"} (horizontal tabs)
    \item Line control: \texttt{"\textbackslash n"}, \texttt{"\textbackslash r"}, \texttt{"\textbackslash n\textbackslash n"}, \texttt{"\textbackslash r\textbackslash n"} (line feeds, carriage returns)
    \item Form control: \texttt{"\textbackslash x0b"}, \texttt{"\textbackslash x0c"} (vertical tab, form feed)
    \item Mixed sequences: \texttt{"\textbackslash t "}, \texttt{" \textbackslash n"}, \texttt{"\textbackslash n  "} (realistic whitespace combinations)
\end{itemize}

For whitespace sequences exceeding the length limit $L_{\max}$ or not present in the learned vocabulary, Viterbi segmentation naturally decomposes them into multiple smaller whitespace tokens. For example, a 12-space indentation might segment as $[\texttt{"    "}, \texttt{"    "}, \texttt{"    "}]$ if 4-space tokens are frequent, or $[\texttt{"  "}, \texttt{"  "}, \texttt{"  "}, \texttt{"  "}, \texttt{"  "}, \texttt{"  "}]$ if 2-space tokens are more common in the corpus.

\paragraph{Consistent Tokenization}

This approach ensures that text like \texttt{"function(x, y)"} is consistently tokenized as separate content and spacing units:
\[
\text{Example: } \texttt{"function(x,\ y)"} \rightarrow [\texttt{"function"}, \texttt{"("}, \texttt{"x"}, \texttt{","}, \texttt{" "}, \texttt{"y"}, \texttt{")"}]
\]
where single space characters are atomic tokens, maintaining clear boundaries between semantic content and structural spacing.

\subsubsection{Case Handling}

To ensure robust matching across varied case patterns in real-world text, we provide configurable case handling strategies during both vocabulary induction and inference. The system supports two modes: \textbf{normalization} (default) and \textbf{preservation}. In normalization mode, all text is converted to lowercase following successful practices in models like BERT-uncased and sentence-transformers, preventing vocabulary fragmentation due to case variations while maintaining semantic meaning. In preservation mode, original casing is maintained for applications where case distinctions carry semantic importance, such as code analysis or proper noun identification. The selected strategy is applied consistently throughout the pipeline during both vocabulary construction and inference, ensuring consistent piece matching across training and inference phases.

\subsubsection{Unigram LM via EM}

We initialize the piece probabilities by normalizing raw frequency counts:
\[
p^{(0)}(u)
= \frac{\mathrm{freq}(u)}{\sum_{v\in\mathcal U_{0}}\mathrm{freq}(v)},
\quad
\forall\,u\in\mathcal U_{0}.
\]

At each EM iteration \(t\), we alternate:

\paragraph{E-step:}

For each sequence \(x\in\mathcal X\), we compute the posterior usage mass \(\gamma^{(t)}(u \mid x)\) for every piece \(u\in \mathcal U_t\). Since enumerating all segmentations of \(x\) is intractable, we approximate using the single best segmentation:
\begin{equation}
\mathrm{seg}^{*}(x)
= \arg\max_{\mathrm{seg}} \prod_{v\in\mathrm{seg}} p^{(t)}(v),
\label{eq:viterbi_decode}
\end{equation}
obtained via Viterbi decoding \cite{kudo2018sentencepiece}. We then compute:
\[
\gamma^{(t)}(u \mid x)
= \sum_{v \in \mathrm{seg}^{*}(x)} \mathbf{1}_{u = v}.
\]

\paragraph{M-step:}

We re-estimate the piece probabilities as:
\begin{equation}
p^{(t+1)}(u)
= \frac{
	\sum_{x \in \mathcal X} \gamma^{(t)}(u \mid x)
}{
	\sum_{v \in \mathcal U_t} \sum_{x \in \mathcal X} \gamma^{(t)}(v \mid x)
},
\label{eq:m_step_update}
\end{equation}
which reduces to relative frequency over the Viterbi segmentations.

\subsubsection{Baseline Perplexity}

To benchmark compressibility before pruning, we compute the initial corpus perplexity using piece-level normalization for consistency with the pruning criterion:
\begin{equation}
\mathrm{PPL}^{(0)}
= \exp\left(
\frac{L^{(0)}}{N_{p}^{(0)}}
\right),
\label{eq:baseline_perplexity}
\end{equation}
where:
\[
L^{(0)} = -\sum_{x\in\mathcal X}\;\sum_{v\in\mathrm{seg}^{*}(x)}\log p^{(0)}(v)
\quad\text{(negative log-likelihood of all pieces)},
\]
\[
N_{p}^{(0)} = \sum_{x\in\mathcal X} \bigl|\mathrm{seg}^{*}(x)\bigr|
\quad\text{(total number of pieces)},
\]
and \(\mathrm{seg}^{*}(x)\) is the Viterbi segmentation under \(p^{(0)}\).

\paragraph{Mathematical Consistency Note:} 
This piece-level normalization ensures that the baseline perplexity \(\mathrm{PPL}^{(0)}\) and pruning perplexity \(\mathrm{PPL}'\) use the same normalization scheme, enabling meaningful comparison in the pruning criterion \(\mathrm{PPL}' - \mathrm{PPL}^{(t)} < \tau_{\mathrm{ppl}}\). An alternative sequence-level baseline would divide by \(|\mathcal X|\) instead of \(N_{p}^{(0)}\), but this would make the pruning comparison mathematically invalid since \(\mathrm{PPL}'\) uses piece-level normalization.

\subsubsection{Adaptive Pruning by PPL and OOV}

After each M‐step, we consider pruning any piece \(u\in V\) with \(p^{(t)}(u)<\epsilon\).  For a candidate removal \(V' = V \setminus \{u\}\), we first perform Viterbi segmentation under the reduced vocabulary:

\[
\mathrm{seg}^{*}_{V'}(x) \;=\;\arg\max_{\mathrm{seg}} \;\prod_{v\in\mathrm{seg}} p^{(t)}_{V'}(v)
\quad\text{for each }x\in\mathcal X.
\]

We then introduce the following quantities:

\[
N_{p}' \;=\;\sum_{x\in\mathcal X} \bigl|\mathrm{seg}^{*}_{V'}(x)\bigr|
\quad\text{(total number of subword pieces)},
\]

\[
L' \;=\; -\,\sum_{x\in\mathcal X}\;\sum_{v\in\mathrm{seg}^{*}_{V'}(x)}\log p^{(t)}_{V'}(v)
\quad\text{(negative log‐likelihood of all pieces)},
\]

\[
N_{t} \;=\;\sum_{x\in\mathcal X} |x|
\quad\text{(total number of codepoint tokens)},
\]

\[
N_{\mathrm{uncov}}' 
\;=\; \sum_{x\in\mathcal X}\;\sum_{i=1}^{|x|}
\mathbf1\bigl(i\not\in \mathrm{cover}_{V'}(x)\bigr)
\quad\text{(total uncovered positions)},
\]

where \(\mathrm{cover}_{V'}(x)\) is the set of codepoint indices spanned by \(\mathrm{seg}^{*}_{V'}(x)\).

Using these, we define:

\[
\mathrm{PPL}' 
= \exp\Bigl(\tfrac{L'}{N_{p}'}\Bigr),
\quad
\mathrm{OOV}' 
= \frac{N_{\mathrm{uncov}}'}{N_{t}}.
\]

We accept the removal \(u\) if and only if both
\[
\mathrm{PPL}' - \mathrm{PPL}^{(t)} < \tau_{\mathrm{ppl}}
\quad\text{and}\quad
\mathrm{OOV}' \le \delta_{\mathrm{oov}}.
\]

This criterion guarantees that any pruning step degrades the model’s average piece‐level log‐likelihood by at most \(\tau_{\mathrm{ppl}}\) and introduces no more than \(\delta_{\mathrm{oov}}\) uncovered codepoint positions.

\subsubsection{Existence and Monotonicity Proposition}

Define the feasible set:
\[
\mathcal F(\tau,\delta)
= \left\{\,V\subseteq\mathcal U_{0}\,\middle|\,
\mathrm{PPL}(V)\le \mathrm{PPL}^{(0)}+\tau,\;
\mathrm{OOV}(V)\le \delta
\right\}.
\]

\begin{proposition}[Feasibility and Monotonicity]
	For any \(\tau,\delta \ge 0\), the feasible set \(\mathcal F(\tau,\delta)\) is nonempty. Moreover, if \(\tau' \ge \tau\), \(\delta' \ge \delta\), then:
	\[
	\mathcal F(\tau,\delta) \subseteq \mathcal F(\tau',\delta'),
	\quad
	\Rightarrow
	\quad
	\min_{V\in\mathcal F(\tau',\delta')}|V| \le \min_{V\in\mathcal F(\tau,\delta)}|V|.
	\]
\end{proposition}

\begin{proof}
	We prove both non-emptiness of the feasible set and the monotonicity property through a constructive approach.
	
	\textbf{Step 1: Existence (Non-emptiness)}
	
	We show that \(\mathcal F(\tau,\delta) \neq \emptyset\) for any \(\tau,\delta \ge 0\) by constructing an explicit member.
	
	Choose \(V = \mathcal U_0\), the full candidate set containing all extracted substrings and individual codepoints.
	
	\emph{Substep 1.1: OOV Analysis}
	
	By construction of \(\mathcal U_0\), every individual codepoint is included. For any sequence \(x \in \mathcal X\) and position \(i \in \{1,\ldots,|x|\}\), the character \(x_i\) satisfies \(x_i \in \mathcal U_0\). Therefore, Viterbi decoding can always fall back to single-character segmentation:
	\[
	\mathrm{seg}^{*}(x) = [x_1, x_2, \ldots, x_{|x|}] \text{ is always feasible under } \mathcal U_0
	\]
	
	This guarantees complete coverage:
	\[
	\mathrm{cover}_{\mathcal U_0}(x) = \{1, 2, \ldots, |x|\} \text{ for all } x \in \mathcal X
	\]
	
	Hence:
	\[
	N_{\mathrm{uncov}} = \sum_{x\in\mathcal X}\;\sum_{i=1}^{|x|} \mathbf1\bigl(i\not\in \mathrm{cover}_{\mathcal U_0}(x)\bigr) = 0
	\]
	
	Therefore: \(\mathrm{OOV}(\mathcal U_0) = \frac{N_{\mathrm{uncov}}}{N_{t}} = 0\)
	
	\emph{Substep 1.2: Perplexity Analysis}
	
	The baseline perplexity \(\mathrm{PPL}^{(0)}\) is computed using the full vocabulary \(\mathcal U_0\) with initial probability estimates \(p^{(0)}\). Since we use the same vocabulary:
	\[
	\mathrm{PPL}(\mathcal U_0) = \mathrm{PPL}^{(0)}
	\]
	
	\emph{Substep 1.3: Feasibility Verification}
	
	For any \(\tau,\delta \ge 0\):
	\begin{align}
	\mathrm{PPL}(\mathcal U_0) = \mathrm{PPL}^{(0)} &\le \mathrm{PPL}^{(0)} + \tau \\
	\mathrm{OOV}(\mathcal U_0) = 0 &\le \delta
	\end{align}
	
	Therefore: \(\mathcal U_0 \in \mathcal F(\tau,\delta)\), proving non-emptiness.
	
	\textbf{Step 2: Monotonicity (Subset Inclusion)}
	
	Let \(\tau' \ge \tau\) and \(\delta' \ge \delta\). We show \(\mathcal F(\tau,\delta) \subseteq \mathcal F(\tau',\delta')\).
	
	Take any \(V \in \mathcal F(\tau,\delta)\). By definition of feasible sets:
	\begin{align}
	\mathrm{PPL}(V) &\le \mathrm{PPL}^{(0)} + \tau \\
	\mathrm{OOV}(V) &\le \delta
	\end{align}
	
	Since \(\tau' \ge \tau\) and \(\delta' \ge \delta\):
	\begin{align}
	\mathrm{PPL}(V) &\le \mathrm{PPL}^{(0)} + \tau \le \mathrm{PPL}^{(0)} + \tau' \\
	\mathrm{OOV}(V) &\le \delta \le \delta'
	\end{align}
	
	Therefore: \(V \in \mathcal F(\tau',\delta')\), establishing the inclusion.
	
	\textbf{Step 3: Minimality (Optimal Vocabulary Size)}
	
	From Step 2, we have the subset relation:
	\[
	\mathcal F(\tau,\delta) \subseteq \mathcal F(\tau',\delta')
	\]
	
	Since \(\mathcal F(\tau,\delta)\) is a non-empty subset of \(\mathcal F(\tau',\delta')\), the minimum vocabulary size over the larger set cannot exceed the minimum over the smaller set:
	\[
	\min_{V \in \mathcal F(\tau',\delta')} |V| \le \min_{V \in \mathcal F(\tau,\delta)} |V|
	\]
	
	This completes the proof of monotonicity in optimal vocabulary size with respect to constraint relaxation.
\end{proof}

\subsubsection{Algorithm: Adaptive Vocabulary Induction}

To construct an efficient and corpus-aware subword vocabulary, we apply an expectation-maximization loop over the candidate set \(\mathcal{U}_0\), guided by a Viterbi-decoded likelihood objective. Starting from raw frequency estimates, we refine piece probabilities \(p(u)\) via best-segmentation token counts and normalize them across the corpus. After each M-step, we attempt to prune low-probability pieces while preserving compressibility and coverage. Each candidate removal is simulated by resegmenting \(\mathcal{X}\) under the reduced vocabulary \(V'\) and evaluating both perplexity and position-level OOV rate. If the degradation in log-likelihood is bounded and no coverage gaps are introduced, the removal is accepted. This selective pruning yields a compact, entropy-regularized vocabulary tailored to the domain structure.

\begin{algorithm}[H]
	\caption{Adaptive Unigram‐LM Vocabulary Induction}
	\label{alg:vocab-induction}
	\begin{algorithmic}[1]
		\STATE Extract candidate substrings up to length \(L_{\max}\) from corpus \(\mathcal{X}\); form initial vocabulary \(\mathcal{U}_0\)
		\STATE Initialize piece probabilities: \(p^{(0)}(u) \propto \mathrm{freq}(u)\) for all \(u \in \mathcal{U}_0\)
		\STATE Compute baseline perplexity \(\mathrm{PPL}^{(0)} = \exp(L^{(0)}/N_p^{(0)})\) via Viterbi decoding over \(\mathcal{X}\)
		\FOR{iteration \(t = 0\) to \(T_{\max}\)}
		\FOR{each sequence \(x \in \mathcal{X}\)}
		\STATE Compute best segmentation \(\mathrm{seg}^{*}(x)\) using current \(p^{(t)}\)
		\STATE Accumulate token usage counts \(\gamma^{(t)}(u \mid x)\) for all \(u \in \mathrm{seg}^{*}(x)\)
		\ENDFOR
		\STATE Update probabilities: normalize counts to get \(p^{(t+1)}(u)\)
		\STATE Set current vocabulary \(V = \{u \mid p^{(t+1)}(u) > 0\}\)
		\FOR{each \(u \in V\) with \(p^{(t+1)}(u) < \epsilon\)}
		\STATE Tentatively prune: define \(V' = V \setminus \{u\}\)
		\FOR{each sequence \(x \in \mathcal{X}\)}
		\STATE Decode \(\mathrm{seg}^{*}_{V'}(x)\) using updated \(V'\)
		\STATE Compute log-prob score and uncovered positions
		\ENDFOR
		\STATE Compute \(\mathrm{PPL}'\) and \(\mathrm{OOV}'\) from new segmentations
		\IF{\(\mathrm{PPL}' - \mathrm{PPL}^{(t)} < \tau_{\mathrm{ppl}}\) and \(\mathrm{OOV}' \le \delta_{\mathrm{oov}}\)}
		\STATE Accept removal: \(V \leftarrow V'\)
		\ENDIF
		\ENDFOR
		\STATE Update \(\mathrm{PPL}^{(t+1)}\) using accepted vocabulary \(V\)
		\ENDFOR
		\STATE \textbf{Return} final pruned vocabulary \(V\) and piece probabilities \(\{p(u)\}\)
	\end{algorithmic}
\end{algorithm}