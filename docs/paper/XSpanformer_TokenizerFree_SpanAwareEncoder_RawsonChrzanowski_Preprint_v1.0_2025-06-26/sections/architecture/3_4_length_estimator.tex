\subsection{Length Estimator}
\label{sec:length-estimator}

Even high-confidence boundary proposals can yield spans that are too short or too long to reflect meaningful linguistic units. To inject a learned prior over span width, we train a categorical length estimator that filters candidates based on predicted versus actual length.

\paragraph{Span Length and Pooling}

For each candidate span \((i,j)\in S\), define the true length:
\[
\delta = j - i + 1.
\]
We pool the contextual embeddings over the span window:
\[
v_{ij} = \mathrm{Pool}\bigl(H[i{:}j]\bigr)\in\mathbb{R}^d,
\]
where \(\mathrm{Pool}(\cdot)\) may be mean-, max-, gated-, or self-attentive aggregation \cite{tay2021charformer}.

\paragraph{Length Prediction}

We predict a categorical distribution over \(B\) discrete length bins:
\[
\ell^\delta = W_\ell\,v_{ij} + b_\ell,
\quad
p^\delta = \softmax(\ell^\delta),
\quad
\hat\delta = \arg\max p^\delta,
\]
where \(\hat\delta\) serves as a learned prior on plausible span width.

\paragraph{Tolerance-Based Filtering}

We retain only spans whose true length \(\delta\) lies within a tolerance \(\tau\) of the predicted bin:
\[
S' = \bigl\{(i,j)\in S \mid |(j-i+1) - \hat\delta| \le \tau \bigr\}.
\]
The hyperparameter \(\tau\) determines how strictly the length prediction must match the actual span width.

\begin{proposition}[Span Count Upper Bound]
	\label{prop:span-length-bound}
	Assume all learned lengths \(\hat\delta\in[\delta_{\min},\delta_{\max}]\) and fix a tolerance \(\tau<\delta_{\max}-\delta_{\min}\). Then the total number of retained spans satisfies:
	\[
	|S'| = \mathcal{O}(T\cdot(2\tau+1)),
	\]
	i.e., only \(O(T)\) spans are retained per input sequence.
\end{proposition}

\begin{proof}
	We analyze the filtering region induced by length tolerance:
	\begin{enumerate}
		\item For a fixed start position \(i\), the predicted length is \(\hat\delta\).  
		\item The allowable end index \(j\) satisfies:
		\[
		j = i + \hat\delta - 1 \pm \tau
		\quad\Rightarrow\quad
		j\in[i + \hat\delta - \tau - 1,\; i + \hat\delta + \tau - 1].
		\]
		\item Thus, for each \(i\), the number of valid \(j\) positions is at most:
		\[
		(i + \hat\delta + \tau - 1) - (i + \hat\delta - \tau - 1) + 1 = 2\tau + 1.
		\]
		\item There are \(T\) possible start positions \(i\), so:
		\[
		|S'| \le T \cdot (2\tau + 1) = \mathcal{O}(T).
		\]
	\end{enumerate}
\end{proof}

This learned filtering module reduces the subquadratic span candidate space to linear size, while enforcing cognitively motivated length regularity \cite{jackendoff1977xbar}. The tolerance parameter \(\tau\) controls the trade-off between structural fidelity and coverage, improving both efficiency and interpretability.