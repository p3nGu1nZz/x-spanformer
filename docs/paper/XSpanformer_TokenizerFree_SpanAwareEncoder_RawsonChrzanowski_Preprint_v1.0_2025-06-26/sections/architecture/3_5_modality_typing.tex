\subsection{Modality Typing}
\label{sec:modality-typing}

Text streams often mix multiple domains—natural language, code syntax, identifiers, numeric values, markup—especially in technical or hybrid contexts. To model this structure, we attach a modality classifier to each pooled span embedding \(v_{ij}\in\mathbb{R}^d\), as defined in Sec.~\ref{sec:length-estimator}. The classifier predicts a soft type distribution and accompanying entropy score.

\subsubsection{Modality Classification}

Let \(M\) be the number of modality classes. We first transform the span vector:
\[
h_{ij} = \mathrm{ReLU}(W_v\,v_{ij} + b_v)\in\mathbb{R}^{d'},
\]
where \(d'\le d\). We then compute:
\[
\ell^{\mathrm{mod}}_{ij} = W_{\mathrm{mod}}\,h_{ij} + b_{\mathrm{mod}},
\quad
p^{\mathrm{mod}}_{ij} = \softmax(\ell^{\mathrm{mod}}_{ij}) \in \Delta^M.
\]
Here \(p^{\mathrm{mod}}_{ij,m}\) is the model’s belief that span \((i,j)\) belongs to modality \(m\).

\subsubsection{Modality Entropy and Ambiguity}

To quantify ambiguity:
\[
H^{\mathrm{mod}}_{ij}
= -\sum_{m=1}^M
p^{\mathrm{mod}}_{ij,m}\,\log p^{\mathrm{mod}}_{ij,m}.
\]
Higher entropy indicates uncertainty, useful for early training exploration or hybrid domains.

\subsubsection{Auxiliary Supervision}

When gold labels \(y^{\mathrm{gold}}_{ij}\in\{0,1\}^M\) are available, we optimize:
\[
\mathcal{L}_{\mathrm{mod}}
= -\sum_{(i,j)\in S}
\sum_{m=1}^M
y^{\mathrm{gold}}_{ij,m}\,\log p^{\mathrm{mod}}_{ij,m}.
\]
This improves zero-shot transfer across modalities \cite{khashabi2020unifiedqa,gupta2022molt}.

\subsubsection{Conditional Routing}

During inference, \(p^{\mathrm{mod}}_{ij}\) can modulate decoder behavior:
\[
e^{\mathrm{mod}}_{ij} = p^{\mathrm{mod}}_{ij} \cdot E_{\mathrm{mod}},
\quad
E_{\mathrm{mod}}\in\mathbb{R}^{M\times d}.
\]
This embedding may be concatenated to \(v_{ij}\) or used to bias attention heads \cite{li2021prefix}.

\subsubsection{Interpretability}

The modality entropy and distribution expose latent semantic domains of each span, aiding mixed‐modality diagnostics and span alignment analysis \cite{lin2021codemix, tay2021charformer}.

\subsubsection{Integration into Span Scoring}

We incorporate modality typing into the span relevance MLP \(f_{\mathrm{score}}\) (Sec.~\ref{sec:span-interpolation}). Define:
\[
d_{ij} = j - i + 1,
\quad
c_{ij} = p^s_i\,p^e_j.
\]
Let \(\phi(d_{ij})\in\mathbb{R}^D\) be a learned embedding of span length. Form the joint feature vector:
\[
x_{ij}
= \bigl[
v_{ij};
\phi(d_{ij});
p^{\mathrm{mod}}_{ij};
H^{\mathrm{mod}}_{ij};
c_{ij}
\bigr]
\in \mathbb{R}^{d + D + M + 1 + 1}.
\]
We compute relevance weight:
\[
w_{ij} = \mathrm{MLP}_{\mathrm{score}}(x_{ij}),
\quad
a_{ij} = \frac{\exp(w_{ij})}{\sum_{(p,q)\in S'}\exp(w_{pq})}.
\]

\begin{proposition}[Modality‐Aware Relevance Weighting]
	Let \(x_{ij}\in\mathbb{R}^d\) be a span descriptor including content, length, boundary confidence, and modality entropy.  Then the normalized weight
	\[
	a_{ij}
	= \frac{\exp\bigl(\mathrm{MLP}(x_{ij})\bigr)}
	{\sum_{(p,q)\in S'} \exp\bigl(\mathrm{MLP}(x_{pq})\bigr)}
	\]
	defines a probability distribution over spans in \(S'\), with preference toward spans that match learned structural and semantic type priors.
\end{proposition}

\begin{proof}
	\textbf{Step 1: Positivity and normalization}
	
	For each \((i,j)\in S'\), let \(w_{ij} = \mathrm{MLP}(x_{ij})\).  Since \(\exp(w_{ij})>0\), it follows immediately that
	\[
	a_{ij} > 0.
	\]
	Moreover,
	\[
	\sum_{(i,j)\in S'} a_{ij}
	= \sum_{(i,j)\in S'} \frac{\exp(w_{ij})}{\sum_{(p,q)\in S'}\exp(w_{pq})}
	= \frac{\sum_{(i,j)\in S'}\exp(w_{ij})}{\sum_{(p,q)\in S'}\exp(w_{pq})}
	= 1.
	\]
	
	\textbf{Step 2: Preference via logits}
	
	Because \(a_{ij}\) is obtained by applying the softmax to the logits \(\{w_{pq}\}\), it is strictly increasing in its own logit and decreasing in the others.  Concretely, if for two spans
	\(\;w_{ij} > w_{p'q'}\), then
	\[
	a_{ij} = \frac{\exp(w_{ij})}{\sum\exp(w)} 
	> \frac{\exp(w_{p'q'})}{\sum\exp(w)} = a_{p'q'}.
	\]
	Since \(w_{ij}=\mathrm{MLP}(x_{ij})\) incorporates modality distribution \(p^{\mathrm{mod}}_{ij}\) and entropy \(H^{\mathrm{mod}}_{ij}\), spans with lower entropy and stronger modality alignment receive higher \(w_{ij}\) and thus larger \(a_{ij}\).
	
	\textbf{Step 3: Explicit softmax form}
	
	Combining the above, we recover the stated formula
	\[
	a_{ij}
	= \frac{\exp\bigl(\mathrm{MLP}(x_{ij})\bigr)}
	{\sum_{(p,q)\in S'} \exp\bigl(\mathrm{MLP}(x_{pq})\bigr)},
	\]
	which is a valid probability distribution over \(S'\) that embeds learned modality‐aware preferences.
\end{proof}