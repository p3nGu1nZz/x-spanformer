\begin{abstract}
	Tokenization remains a fundamental limitation in transformer architectures, as static subword vocabularies fragment cross-domain text and impede adaptability. We propose X-Spanformer, a tokenizer-free segmentation module inspired by X-bar theory that begins with a hybrid Unigram-LM vocabulary—comprising all UTF-8 characters plus top entropy-pruned subword units—computed as a soft probability matrix \(P\in\mathbb{R}^{T\times V}\) via a custom ONNX operator \cite{kudo2018sentencepiece,rawson2025streammix}. A factorized pointer network locates variable-length, overlapping spans \cite{vinyals2015pointer}, which are softly typed by modality, filtered by a learned length estimator, and pooled into \(d\)-dimensional embeddings. Retained spans are fused into a single controller vector \(s\) through relevance-weighted interpolation and injected into downstream transformer encoders using prefix-token, attention-bias, or gated-FFN pathways. The model is trained with an entropy-regularized span induction curriculum that transitions from synthetic supervision to type-aware signals. We evaluate on compression ratio, contrastive retrieval alignment, span entropy, and modality coherence, demonstrating improved structural interpretability and compression efficiency over static BPE and byte-level baselines. An ONNX-compatible implementation, along with training recipes and corpus construction guidelines, is released to support adoption across code, language, and hybrid domains.
\end{abstract}