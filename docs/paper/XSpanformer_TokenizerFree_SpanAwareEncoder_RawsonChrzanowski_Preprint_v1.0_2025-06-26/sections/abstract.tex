\begin{abstract}
Tokenization remains a limiting factor in contemporary transformer architectures, typically grounded in static subword vocabularies that generalize poorly across heterogeneous or evolving textual inputs. We introduce \textbf{X-Spanformer}, a tokenizer-free segmentation module that replaces heuristic lexical boundaries with dynamic span prediction inspired by X-bar theory. The model initializes with a compact 1k-unit BPE vocabulary \cite{sennrich2016bpe,kudo2018sentencepiece} and transitions to span-based segmentation via a pointer network \cite{vinyals2015pointer}, supervised through an annealed auxiliary loss schedule. Predicted spans are variable-length, overlapping, softly typed by modality (e.g., code, natural language, identifier), and dynamically capped per input using a learned span length estimator. Span representations are composed and injected into downstream encoders, allowing seamless integration with pretrained or co-trained transformer stacks. We hypothesize that learned span prediction provides more semantically aligned and compression-efficient tokenization than fixed BPE or byte-level alternatives. To investigate this, we construct a multi-phase curriculum that bootstraps from synthetic segmentation labels and gradually introduces stream-typeâ€“aligned supervision. We detail evaluation protocols for measuring compression ratio, contrastive retrieval alignment, span entropy, and modality coherence. Preliminary results suggest improved interpretability and structural alignment, supporting our hypothesis. We release a standalone ONNX-compatible implementation along with training recipes and corpus construction guidelines to facilitate adoption across code, language, and hybrid domains.
\end{abstract}