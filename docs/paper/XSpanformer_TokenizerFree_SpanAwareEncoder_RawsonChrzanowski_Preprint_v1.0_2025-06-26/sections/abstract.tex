\begin{abstract}
	Tokenization remains a fundamental bottleneck in transformer architectures, as static subword vocabularies fragment cross-domain text and impede real-time adaptability. We introduce X-Spanformer, a tokenizer-free, span-aware segmentation front end inspired by X-bar theory. X-Spanformer begins by inducing a hybrid Unigram-LM vocabulary—comprising all UTF-8 codepoints plus top entropy-pruned subword fragments—via an EM procedure approximated with Viterbi decoding, then applies a learned pruning criterion based on per-piece perplexity and OOV rate; this entire process is implemented as an efficient ONNX custom operator. The resulting soft piece-assignment matrix \(P\in\mathbb{R}^{T\times V}\) is embedded and contextualized through a lightweight convolutional encoder that scales linearly in \(T\). A factorized pointer network then identifies overlapping, variable-length spans, which are softly typed by modality, filtered by a learned length estimator, and pooled into \(d\)-dimensional span embeddings. We retain the top-\(K\) spans and fuse them via relevance-weighted interpolation into a single controller vector \(s\), which is injected into downstream transformer layers through prefix-token, attention-bias, or gated fusion pathways. X-Spanformer is trained with an entropy-regularized span induction curriculum transitioning from synthetic supervision to type-aware signals. Empirical evaluation on compression ratio, contrastive retrieval alignment, span entropy, modality coherence, and end-to-end inference speed demonstrates that X-Spanformer outperforms static BPE and byte-level baselines in structural interpretability, compression efficiency, and throughput. An open-source ONNX-compatible implementation, along with training recipes and corpus construction guidelines, is provided to facilitate adoption across code, language, and hybrid domains.
\end{abstract}
