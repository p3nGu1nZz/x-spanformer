# ğŸ§  X-Spanformer

**Tokenizer-free, span-aware transformer grounded in X-bar theory**  
X-Spanformer learns to segment and fuse overlapping spans directly from raw inputâ€”code, natural language, or hybrid stringsâ€”without relying on tokenizers. It uses span-induced controller vectors to steer computation and model structure in a linguistically grounded, modality-flexible way.

---

## ğŸš€ Key Features

- **Tokenizer-Free Encoding** â€“ no subword splits or external segmenters  
- **Span-Aware Attention Routing** â€“ structure is learned and fused into prefix/control signals  
- **Multi-Domain Compositionality** â€“ supports code, prose, REPLs, Markdown, etc.  
- **Entropy-Annealed Training** â€“ spans are initially exploratory, then crystallize over time  
- **X-Bar Inspired Representation** â€“ spans learned hierarchically, with linguistic roles and projections  

---

## ğŸ“¦ Data Format

Training examples follow this schema:

```json
{
  "input": ["The", " ", "quick", " ", "brown", " ", "fox", "."],
  "type": "natural_language",
  "span_labels": [
    { "span": [0, 0], "label": "determiner", "role": "specifier", "text": "The" },
    { "span": [2, 4], "label": "adjective_phrase", "role": "modifier", "text": "quick brown" },
    { "span": [6, 6], "label": "noun", "role": "subject", "text": "fox" }
  ]
}
```

For full details, see [`/examples`](./examples) and the companion compiler agent [`ox-bar`](https://github.com/.../ox-bar).

---

## ğŸ§ª Data Preprocessing

Our preprocessing pipeline consists of two main stages:

### Stage 1: PDF to JSONL Conversion

To generate semantically coherent pretraining data without tokenizers, we use the [`pdf2seg`](https://pypi.org/project/pdf2seg) package:

```bash
pip install pdf2seg
```

Process scanned or structured PDFs into entropy-minimized text spans:

```bash
# Generate JSONL segments from PDFs
uv run -m x_spanformer.pipelines.pdf2jsonl \
  -i input_pdfs/ \
  -o data/pretraining/out \
  --name pretraining
```

### Stage 2: Vocabulary Induction

Generate a hybrid Unigram-LM vocabulary from the JSONL segments:

```bash
# Induce vocabulary from JSONL segments
uv run -m x_spanformer.pipelines.jsonl2vocab \
  -i data/pretraining/out \
  -o data/vocab/out
```

This implements the mathematical formulation from Section 3.1 of our paper, using EM + Viterbi segmentation with adaptive pruning based on perplexity and OOV thresholds.

Use the output as either raw training strings (for unsupervised Phase I) or compile with `oxbar` to produce labeled span records.

This enables X-Spanformer to bootstrap span boundaries from real-world documents with high structural signal, without relying on brittle tokenization.

---

## ğŸ§° Repository Structure

```
x-spanformer/
â”œâ”€â”€ x_spanformer/
â”‚   â”œâ”€â”€ pipelines/        # Data processing pipelines
â”‚   â”‚   â”œâ”€â”€ pdf2jsonl.py  # PDF â†’ JSONL conversion with AI judging
â”‚   â”‚   â””â”€â”€ jsonl2vocab.py # Hybrid Unigram-LM vocabulary induction
â”‚   â”œâ”€â”€ schema/           # Pydantic data models and validation
â”‚   â”‚   â”œâ”€â”€ pretrain_record.py # Training data schema
â”‚   â”‚   â”œâ”€â”€ vocab.py      # Vocabulary piece and statistics schemas
â”‚   â”‚   â””â”€â”€ ...           # Other schema definitions
â”‚   â”œâ”€â”€ agents/           # AI agents for content judging and processing
â”‚   â”œâ”€â”€ controllers/      # Span controller logic
â”‚   â””â”€â”€ views/            # Data visualization and inspection
â”œâ”€â”€ config/               # Pipeline configurations
â”‚   â””â”€â”€ pipelines/        # YAML configs for data processing
â”œâ”€â”€ data/                 # Training and vocabulary data
â”‚   â”œâ”€â”€ pretraining/      # Raw segments from PDF processing
â”‚   â””â”€â”€ vocab/            # Vocabulary induction outputs
â”œâ”€â”€ docs/                 # Documentation and paper materials
â”‚   â””â”€â”€ paper/            # LaTeX source and compiled paper
â”œâ”€â”€ tests/                # Unit tests and integration tests
â””â”€â”€ examples/             # Sample data and usage examples
```

---

## ğŸ§ª Pipeline Tools

### Core Pipelines

- **`pdf2jsonl.py`** â€” Convert PDFs to validated JSONL segments with AI content judging
- **`jsonl2vocab.py`** â€” Induce hybrid Unigram-LM vocabulary using EM + Viterbi with adaptive pruning

### Validation & Analysis

- **Schema validation** â€” Pydantic models ensure data consistency across pipelines
- **Rich console output** â€” Detailed progress tracking and statistics reporting
- **Incremental processing** â€” Resume interrupted runs and process new data efficiently

### Configuration

- **YAML-based configs** â€” Hyperparameter tuning for vocabulary induction and content judging
- **Modular architecture** â€” Easy to extend with new processing stages and validation rules  

---

## ğŸ”§ External Tools

### [`pdf2seg`](https://pypi.org/project/pdf2seg)

Segment PDF documents into structured clauses using OCR + spaCy:

```bash
pdf2seg -i paper.pdf -o spans/
```

Ideal for extracting domain-specific clause boundaries from scientific papers, REPL transcripts, or code-heavy PDFs. The output is then processed by our `pdf2jsonl` pipeline for validation and schema conformance.

### [`oxbar`](https://github.com/.../ox-bar)

Generate structured span-labeled records using local LLMs:

```bash
oxbar compile input.txt --type mixed --output spans.json
```

Supports retry logic, confidence scoring, and mode switching. Complements our vocabulary induction by providing supervised span labels for training data.

---

## ğŸ§¬ Architectural Inspiration

- **Linguistics:** X-bar phrase theory, projection-based syntax, span recursion  
- **Biomimicry:** Mycelial routing, compositional inference, entropy-driven adaptation  
- **Transformer Augmentation:** Span-aware attention, controller modulation, dropout-driven routing  

---

## ğŸ¤ Contributing

We welcome span explorers, linguistically curious devs, and tokenizer skeptics.

Ways to help:
- Label new examples using `oxbar` or manual annotations  
- Extend the span role taxonomy for underrepresented domains (e.g., REPLs, math, RST)  
- Build new controller fusion heads or injection pathways  
- Analyze span induction across language families, treebanks, or doc formats  
- Visualize structural routing dynamics in longer sequences

Start with [`CONTRIBUTING.md`](./CONTRIBUTING.md) to onboard.

---

## ğŸ“„ Citation & License

This research and code are licensed under the **Creative Commons Attribution 4.0 International (CC BY 4.0)**.

```
Copyright (c) 2025  
TAU SYSTEMS by NAXZYU CORP.
```

### ğŸ“š Zenodo Preprint  

**[https://zenodo.org/records/15750962](https://zenodo.org/records/15750962)**

```bibtex
@misc{rawson2025xspanformer,
  title        = {X-Spanformer: Tokenizer-Free Span Induction with Structural Fusion},
  author       = {Rawson, Kara and Chrzanowski, Aimee},
  year         = {2025},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.15750962},
  url          = {https://zenodo.org/records/15750962}
}
```
